--- mesa-chaotic/src/amd/llvm/ac_llvm_build.c	2025-05-11 17:14:25.520219449 +0200
+++ mesa-dadschoorse/src/amd/llvm/ac_llvm_build.c	2025-05-11 17:09:56.099774669 +0200
@@ -156,7 +156,7 @@
          return 32;
    }
 
-   if (type == ctx->f16 || type == ctx->bf16)
+   if (type == ctx->f16)
       return 16;
    if (type == ctx->f32)
       return 32;
@@ -174,7 +174,6 @@
    case LLVMIntegerTypeKind:
       return LLVMGetIntTypeWidth(type) / 8;
    case LLVMHalfTypeKind:
-   case LLVMBFloatTypeKind:
       return 2;
    case LLVMFloatTypeKind:
       return 4;
@@ -200,7 +199,7 @@
       return ctx->i1;
    else if (t == ctx->i8)
       return ctx->i8;
-   else if (t == ctx->f16 || t == ctx->bf16 || t == ctx->i16)
+   else if (t == ctx->f16 || t == ctx->i16)
       return ctx->i16;
    else if (t == ctx->f32 || t == ctx->i32)
       return ctx->i32;
@@ -359,9 +358,6 @@
    case LLVMIntegerTypeKind:
       snprintf(buf, bufsize, "i%d", LLVMGetIntTypeWidth(elem_type));
       break;
-   case LLVMBFloatTypeKind:
-      snprintf(buf, bufsize, "bf16");
-      break;
    case LLVMHalfTypeKind:
       snprintf(buf, bufsize, "f16");
       break;
--- mesa-chaotic/src/amd/common/nir/ac_nir.c	2025-05-11 17:14:25.512064231 +0200
+++ mesa-dadschoorse/src/amd/common/nir/ac_nir.c	2025-05-11 17:09:56.086770865 +0200
@@ -77,7 +77,6 @@
    options->has_sudot_4x8_sat = info->has_accelerated_dot_product && info->gfx_level >= GFX11;
    options->has_udot_4x8_sat = info->has_accelerated_dot_product;
    options->has_dot_2x16 = info->has_accelerated_dot_product && info->gfx_level < GFX11;
-   options->has_bfdot2_bfadd = info->gfx_level >= GFX12;
    options->has_find_msb_rev = true;
    options->has_pack_32_4x8 = true;
    options->has_pack_half_2x16_rtz = true;
@@ -86,6 +85,7 @@
    options->has_msad = true;
    options->has_shfr32 = true;
    options->has_mul24_relaxed = true;
+   options->has_bfdot2_bfadd = true;
    options->lower_int64_options = nir_lower_imul64 | nir_lower_imul_high64 | nir_lower_imul_2x32_64 | nir_lower_divmod64 |
                                   nir_lower_minmax64 | nir_lower_iabs64 | nir_lower_iadd_sat64 | nir_lower_conv64;
    options->divergence_analysis_options = nir_divergence_view_index_uniform;
@@ -577,14 +577,13 @@
       if (config->uses_aco && uses_smem && aligned_new_size >= 128)
          overfetch_size = 32;
 
-      /* Allow overfetching from 8/16 bits to 32 bits. */
       int64_t aligned_unvectorized_size =
-         ALIGN_POT(align_load_store_size(config->gfx_level, low->num_components * low->def.bit_size,
-                                         uses_smem, is_shared), 32) +
-         ALIGN_POT(align_load_store_size(config->gfx_level, high->num_components * high->def.bit_size,
-                                         uses_smem, is_shared), 32);
+         align_load_store_size(config->gfx_level, low->num_components * low->def.bit_size,
+                               uses_smem, is_shared) +
+         align_load_store_size(config->gfx_level, high->num_components * high->def.bit_size,
+                               uses_smem, is_shared);
 
-      if (ALIGN_POT(aligned_new_size, 32) > aligned_unvectorized_size + overfetch_size)
+      if (aligned_new_size > aligned_unvectorized_size + overfetch_size)
          return false;
    }
 
@@ -605,16 +604,32 @@
 
    /* Validate the alignment and number of components. */
    if (!is_shared) {
-      return (align % (bit_size / 8u)) == 0 && num_components <= NIR_MAX_VEC_COMPONENTS;
+      unsigned max_components;
+      if (align % 4 == 0)
+         max_components = NIR_MAX_VEC_COMPONENTS;
+      else if (align % 2 == 0)
+         max_components = 16u / bit_size;
+      else
+         max_components = 8u / bit_size;
+      return (align % (bit_size / 8u)) == 0 && num_components <= max_components;
    } else {
-      if (bit_size >= 32 && num_components == 3) {
-         /* AMD hardware can't do 3-component loads except for 96-bit loads. */
-         return bit_size == 32 && align % 16 == 0;
+      if (bit_size * num_components == 96) { /* 96 bit loads require 128 bit alignment and are split otherwise */
+         return align % 16 == 0;
+      } else if (bit_size == 16 && (align % 4)) {
+         /* AMD hardware can't do 2-byte aligned f16vec2 loads, but they are useful for ALU
+          * vectorization, because our vectorizer requires the scalar IR to already contain vectors.
+          */
+         return (align % 2 == 0) && num_components <= 2;
+      } else {
+         if (num_components == 3) {
+            /* AMD hardware can't do 3-component loads except for 96-bit loads, handled above. */
+            return false;
+         }
+         unsigned req = bit_size * num_components;
+         if (req == 64 || req == 128) /* 64-bit and 128-bit loads can use ds_read2_b{32,64} */
+            req /= 2u;
+         return align % (req / 8u) == 0;
       }
-      unsigned req = bit_size >= 32 ? bit_size * num_components : bit_size;
-      if (req == 64 || req == 128) /* 64-bit and 128-bit loads can use ds_read2_b{32,64} */
-         req /= 2u;
-      return align % (req / 8u) == 0;
    }
    return false;
 }
--- mesa-chaotic/src/amd/llvm/ac_nir_to_llvm.c	2025-05-11 17:14:25.520779739 +0200
+++ mesa-dadschoorse/src/amd/llvm/ac_nir_to_llvm.c	2025-05-11 17:09:56.100649048 +0200
@@ -1227,18 +1227,11 @@
    }
 
    case nir_op_bfdot2_bfadd: {
-      const char *name = "llvm.amdgcn.fdot2.bf16.bf16";
-      LLVMTypeRef vec2_type = ctx->ac.v2bf16;
-      LLVMTypeRef scalar_type = ctx->ac.bf16;
-#if LLVM_VERSION_MAJOR < 19 || (LLVM_VERSION_MAJOR == 19 && LLVM_VERSION_MINOR == 0)
-      /* Before LLVM 19.1, bf16 fdot used integer operands. */
-      vec2_type = ctx->ac.v2i16;
-      scalar_type = ctx->ac.i16;
-#endif
-      src[0] = LLVMBuildBitCast(ctx->ac.builder, src[0], vec2_type, "");
-      src[1] = LLVMBuildBitCast(ctx->ac.builder, src[1], vec2_type, "");
-      src[2] = LLVMBuildBitCast(ctx->ac.builder, src[2], scalar_type, "");
-      result = ac_build_intrinsic(&ctx->ac, name, scalar_type, src, 3, 0);
+      const char *name = "llvm.amdgcn.fdot2";
+      src[0] = LLVMBuildBitCast(ctx->ac.builder, src[0], ctx->ac.v2bf16, "");
+      src[1] = LLVMBuildBitCast(ctx->ac.builder, src[1], ctx->ac.v2bf16, "");
+      src[2] = LLVMBuildBitCast(ctx->ac.builder, src[2], ctx->ac.bf16, "");
+      result = ac_build_intrinsic(&ctx->ac, name, def_type, src, 3, 0);
       break;
    }
 
--- mesa-chaotic/src/amd/compiler/aco_instruction_selection.cpp	2025-05-11 17:14:25.514443603 +0200
+++ mesa-dadschoorse/src/amd/compiler/aco_instruction_selection.cpp	2025-05-11 17:09:56.092535648 +0200
@@ -598,7 +598,7 @@
          elems[i] = emit_extract_vector(ctx, vec, src.swizzle[i], elem_rc);
          vec_instr->operands[i] = Operand{elems[i]};
       }
-      Temp dst = ctx->program->allocateTmp(RegClass(vec.type(), elem_size * size / 4));
+      Temp dst = ctx->program->allocateTmp(RegClass::get(vec.type(), elem_size * size));
       vec_instr->definitions[0] = Definition(dst);
       ctx->block->instructions.emplace_back(std::move(vec_instr));
       ctx->allocated_vec.emplace(dst.id(), elems);
@@ -2997,6 +2997,35 @@
       bld.vop1(aco_opcode::v_cvt_f64_f32, Definition(dst), src);
       break;
    }
+   case nir_op_f2e4m3fn: {
+      Operand src0, src1;
+      if (instr->def.num_components == 2) {
+         Temp src = get_ssa_temp(ctx, instr->src[0].src.ssa);
+         RegClass rc = RegClass(src.regClass().type(), 1);
+         src0 = Operand(emit_extract_vector(ctx, src, instr->src[0].swizzle[0], rc));
+         src1 = Operand(emit_extract_vector(ctx, src, instr->src[0].swizzle[1], rc));
+      } else {
+         assert(instr->def.num_components == 1);
+         src0 = Operand(get_alu_src(ctx, instr->src[0]));
+         src1 = Operand::c32(0);
+      }
+      bld.vop3(aco_opcode::v_cvt_pk_fp8_f32, Definition(dst), src0, src1);
+      if (instr->def.num_components == 2)
+         emit_split_vector(ctx, dst, 2);
+      break;
+   }
+   case nir_op_e4m3fn2f: {
+      if (instr->def.num_components == 2) {
+         Temp src = get_alu_src(ctx, instr->src[0], 2);
+         bld.vop1(aco_opcode::v_cvt_pk_f32_fp8, Definition(dst), src);
+         emit_split_vector(ctx, dst, 2);
+      } else {
+         Temp src = get_alu_src(ctx, instr->src[0]);
+         assert(instr->def.num_components == 1);
+         bld.vop1(aco_opcode::v_cvt_f32_fp8, Definition(dst), src);
+      }
+      break;
+   }
    case nir_op_i2f16: {
       Temp src = get_alu_src(ctx, instr->src[0]);
       const unsigned input_size = instr->src[0].src.ssa->bit_size;
@@ -4238,7 +4267,7 @@
                              Temp dst_hint);
 
    Callback callback;
-   uint32_t max_const_offset;
+   unsigned max_const_offset_plus_one;
 };
 
 void
@@ -4270,10 +4299,10 @@
       /* reduce constant offset */
       Operand offset = info.offset;
       unsigned reduced_const_offset = const_offset;
-      if (const_offset && const_offset > params.max_const_offset) {
-         uint32_t max_const_offset_plus_one = params.max_const_offset + 1;
-         unsigned to_add = const_offset / max_const_offset_plus_one * max_const_offset_plus_one;
-         reduced_const_offset %= max_const_offset_plus_one;
+      if (const_offset && (const_offset >= params.max_const_offset_plus_one)) {
+         unsigned to_add =
+            const_offset / params.max_const_offset_plus_one * params.max_const_offset_plus_one;
+         reduced_const_offset %= params.max_const_offset_plus_one;
          Temp offset_tmp = offset.isTemp() ? offset.getTemp() : Temp();
          if (offset.isConstant()) {
             offset = Operand::c32(offset.constantValue() + to_add);
@@ -4510,34 +4539,11 @@
 
 const EmitLoadParameters lds_load_params{lds_load_callback, UINT32_MAX};
 
-std::pair<aco_opcode, unsigned>
-get_smem_opcode(amd_gfx_level level, unsigned bytes, bool buffer, bool round_down)
-{
-   if (bytes <= 1 && level >= GFX12)
-      return {buffer ? aco_opcode::s_buffer_load_ubyte : aco_opcode::s_load_ubyte, 1};
-   else if (bytes <= (round_down ? 3 : 2) && level >= GFX12)
-      return {buffer ? aco_opcode::s_buffer_load_ushort : aco_opcode::s_load_ushort, 2};
-   else if (bytes <= (round_down ? 7 : 4))
-      return {buffer ? aco_opcode::s_buffer_load_dword : aco_opcode::s_load_dword, 4};
-   else if (bytes <= (round_down ? (level >= GFX12 ? 11 : 15) : 8))
-      return {buffer ? aco_opcode::s_buffer_load_dwordx2 : aco_opcode::s_load_dwordx2, 8};
-   else if (bytes <= (round_down ? 15 : 12) && level >= GFX12)
-      return {buffer ? aco_opcode::s_buffer_load_dwordx3 : aco_opcode::s_load_dwordx3, 12};
-   else if (bytes <= (round_down ? 31 : 16))
-      return {buffer ? aco_opcode::s_buffer_load_dwordx4 : aco_opcode::s_load_dwordx4, 16};
-   else if (bytes <= (round_down ? 63 : 32))
-      return {buffer ? aco_opcode::s_buffer_load_dwordx8 : aco_opcode::s_load_dwordx8, 32};
-   else
-      return {buffer ? aco_opcode::s_buffer_load_dwordx16 : aco_opcode::s_load_dwordx16, 64};
-}
-
 Temp
 smem_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigned bytes_needed,
                    unsigned align, unsigned const_offset, Temp dst_hint)
 {
-   /* Only scalar sub-dword loads are supported. */
-   assert(bytes_needed % 4 == 0 || bytes_needed <= 2);
-   assert(align >= MIN2(bytes_needed, 4));
+   assert(align >= 4u);
 
    bld.program->has_smem_buffer_or_global_loads = true;
 
@@ -4548,25 +4554,25 @@
       offset = Temp();
    }
 
-   std::pair<aco_opcode, unsigned> smaller =
-      get_smem_opcode(bld.program->gfx_level, bytes_needed, buffer, true);
-   std::pair<aco_opcode, unsigned> larger =
-      get_smem_opcode(bld.program->gfx_level, bytes_needed, buffer, false);
-
+   bytes_needed = MIN2(bytes_needed, 64);
+   unsigned needed_round_up = util_next_power_of_two(bytes_needed);
+   unsigned needed_round_down = needed_round_up >> (needed_round_up != bytes_needed ? 1 : 0);
    /* Only round-up global loads if it's aligned so that it won't cross pages */
-   aco_opcode op;
-   std::tie(op, bytes_needed) =
-      buffer || (align % util_next_power_of_two(larger.second) == 0) ? larger : smaller;
+   bytes_needed = buffer || align % needed_round_up == 0 ? needed_round_up : needed_round_down;
 
-   /* Use a s4 regclass for dwordx3 loads. Even if the register allocator aligned s3 SMEM
-    * definitions correctly, multiple dwordx3 loads can make very inefficient use of the register
-    * file. There might be a single SGPR hole between each s3 temporary, making no space for a
-    * vector without a copy for each SGPR needed. Using a s4 definition instead should help avoid
-    * this situation by preventing the scheduler and register allocator from assuming that the 4th
-    * SGPR of each definition in a sequence of dwordx3 SMEM loads is free for use by vector
-    * temporaries.
-    */
-   RegClass rc(RegType::sgpr, DIV_ROUND_UP(util_next_power_of_two(bytes_needed), 4u));
+   aco_opcode op;
+   if (bytes_needed <= 4) {
+      op = buffer ? aco_opcode::s_buffer_load_dword : aco_opcode::s_load_dword;
+   } else if (bytes_needed <= 8) {
+      op = buffer ? aco_opcode::s_buffer_load_dwordx2 : aco_opcode::s_load_dwordx2;
+   } else if (bytes_needed <= 16) {
+      op = buffer ? aco_opcode::s_buffer_load_dwordx4 : aco_opcode::s_load_dwordx4;
+   } else if (bytes_needed <= 32) {
+      op = buffer ? aco_opcode::s_buffer_load_dwordx8 : aco_opcode::s_load_dwordx8;
+   } else {
+      assert(bytes_needed == 64);
+      op = buffer ? aco_opcode::s_buffer_load_dwordx16 : aco_opcode::s_load_dwordx16;
+   }
 
    aco_ptr<Instruction> load{create_instruction(op, Format::SMEM, 2, 1)};
    if (buffer) {
@@ -4585,24 +4591,16 @@
       else
          load->operands[1] = Operand::c32(const_offset);
    }
-   Temp val = dst_hint.id() && dst_hint.regClass() == rc && rc.bytes() == bytes_needed
-                 ? dst_hint
-                 : bld.tmp(rc);
+   RegClass rc(RegType::sgpr, DIV_ROUND_UP(bytes_needed, 4u));
+   Temp val = dst_hint.id() && dst_hint.regClass() == rc ? dst_hint : bld.tmp(rc);
    load->definitions[0] = Definition(val);
    load->smem().cache = info.cache;
    load->smem().sync = info.sync;
    bld.insert(std::move(load));
-
-   if (rc.bytes() > bytes_needed) {
-      rc = RegClass(RegType::sgpr, DIV_ROUND_UP(bytes_needed, 4u));
-      Temp val2 = dst_hint.id() && dst_hint.regClass() == rc ? dst_hint : bld.tmp(rc);
-      val = bld.pseudo(aco_opcode::p_extract_vector, Definition(val2), val, Operand::c32(0u));
-   }
-
    return val;
 }
 
-const EmitLoadParameters smem_load_params{smem_load_callback, 1023};
+const EmitLoadParameters smem_load_params{smem_load_callback, 1024};
 
 Temp
 mubuf_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigned bytes_needed,
@@ -4668,7 +4666,7 @@
    return val;
 }
 
-const EmitLoadParameters mubuf_load_params{mubuf_load_callback, 4095};
+const EmitLoadParameters mubuf_load_params{mubuf_load_callback, 4096};
 
 Temp
 mubuf_load_format_callback(Builder& bld, const LoadEmitInfo& info, Temp offset,
@@ -4732,7 +4730,7 @@
    return val;
 }
 
-const EmitLoadParameters mubuf_load_format_params{mubuf_load_format_callback, 4095};
+const EmitLoadParameters mubuf_load_format_params{mubuf_load_format_callback, 4096};
 
 Temp
 scratch_load_callback(Builder& bld, const LoadEmitInfo& info, Temp offset, unsigned bytes_needed,
@@ -4772,8 +4770,8 @@
    return val;
 }
 
-const EmitLoadParameters scratch_mubuf_load_params{mubuf_load_callback, 4095};
-const EmitLoadParameters scratch_flat_load_params{scratch_load_callback, 2047};
+const EmitLoadParameters scratch_mubuf_load_params{mubuf_load_callback, 4096};
+const EmitLoadParameters scratch_flat_load_params{scratch_load_callback, 2048};
 
 Temp
 get_gfx6_global_rsrc(Builder& bld, Temp addr)
@@ -5694,7 +5692,7 @@
    return val;
 }
 
-const EmitLoadParameters mtbuf_load_params{mtbuf_load_callback, 4095};
+const EmitLoadParameters mtbuf_load_params{mtbuf_load_callback, 4096};
 
 void
 visit_load_fs_input(isel_context* ctx, nir_intrinsic_instr* instr)
@@ -5781,12 +5779,12 @@
             Temp rsrc, Temp offset, unsigned align_mul, unsigned align_offset,
             unsigned access = ACCESS_CAN_REORDER, memory_sync_info sync = memory_sync_info())
 {
+   assert(!(access & ACCESS_SMEM_AMD) || (component_size >= 4));
+
    Builder bld(ctx->program, ctx->block);
 
    bool use_smem = access & ACCESS_SMEM_AMD;
    if (use_smem) {
-      assert(component_size >= 4 ||
-             (num_components * component_size <= 2 && ctx->program->gfx_level >= GFX12));
       offset = bld.as_uniform(offset);
    } else {
       /* GFX6-7 are affected by a hw bug that prevents address clamping to
@@ -6708,14 +6706,13 @@
 
    unsigned access = nir_intrinsic_access(instr) | ACCESS_TYPE_LOAD;
    if (access & ACCESS_SMEM_AMD) {
-      assert(component_size >= 4 ||
-             (num_components * component_size <= 2 && ctx->program->gfx_level >= GFX12));
+      assert(component_size >= 4);
       if (info.resource.id())
          info.resource = bld.as_uniform(info.resource);
       info.offset = Operand(bld.as_uniform(info.offset));
       info.cache = get_cache_flags(ctx, access | ACCESS_TYPE_SMEM);
       EmitLoadParameters params = smem_load_params;
-      params.max_const_offset = ctx->program->dev.smem_offset_max;
+      params.max_const_offset_plus_one = ctx->program->dev.smem_offset_max + 1;
       emit_load(ctx, bld, info, params);
    } else {
       EmitLoadParameters params = global_load_params;
@@ -7032,7 +7029,7 @@
       info.split_by_component_stride = false;
 
       EmitLoadParameters params = mtbuf_load_params;
-      params.max_const_offset = ctx->program->dev.buf_offset_max;
+      params.max_const_offset_plus_one = ctx->program->dev.buf_offset_max + 1;
       emit_load(ctx, bld, info, params);
    } else {
       assert(intrin->intrinsic == nir_intrinsic_load_buffer_amd);
@@ -7041,7 +7038,7 @@
          assert(!swizzled);
 
          EmitLoadParameters params = mubuf_load_format_params;
-         params.max_const_offset = ctx->program->dev.buf_offset_max;
+         params.max_const_offset_plus_one = ctx->program->dev.buf_offset_max + 1;
          emit_load(ctx, bld, info, params);
       } else {
          const unsigned swizzle_element_size =
@@ -7053,7 +7050,7 @@
          info.align_offset = align_offset;
 
          EmitLoadParameters params = mubuf_load_params;
-         params.max_const_offset = ctx->program->dev.buf_offset_max;
+         params.max_const_offset_plus_one = ctx->program->dev.buf_offset_max + 1;
          emit_load(ctx, bld, info, params);
       }
    }
@@ -7141,16 +7138,28 @@
                         Operand::c32(ctx->options->address32_hi));
    }
 
-   aco_opcode opcode;
-   unsigned size;
+   aco_opcode opcode = aco_opcode::s_load_dword;
+   unsigned size = 1;
+
    assert(dst.bytes() <= 64);
-   std::tie(opcode, size) = get_smem_opcode(ctx->program->gfx_level, dst.bytes(), false, false);
-   size = util_next_power_of_two(size);
 
-   if (dst.size() != DIV_ROUND_UP(size, 4)) {
+   if (dst.bytes() > 32) {
+      opcode = aco_opcode::s_load_dwordx16;
+      size = 16;
+   } else if (dst.bytes() > 16) {
+      opcode = aco_opcode::s_load_dwordx8;
+      size = 8;
+   } else if (dst.bytes() > 8) {
+      opcode = aco_opcode::s_load_dwordx4;
+      size = 4;
+   } else if (dst.bytes() > 4) {
+      opcode = aco_opcode::s_load_dwordx2;
+      size = 2;
+   }
+
+   if (dst.size() != size) {
       bld.pseudo(aco_opcode::p_extract_vector, Definition(dst),
-                 bld.smem(opcode, bld.def(RegClass::get(RegType::sgpr, size)), base, offset),
-                 Operand::c32(0u));
+                 bld.smem(opcode, bld.def(RegType::sgpr, size), base, offset), Operand::c32(0u));
    } else {
       bld.smem(opcode, Definition(dst), base, offset);
    }
@@ -7546,7 +7555,7 @@
          info.offset = Operand(get_ssa_temp(ctx, instr->src[0].ssa));
       }
       EmitLoadParameters params = scratch_flat_load_params;
-      params.max_const_offset = ctx->program->dev.scratch_global_offset_max;
+      params.max_const_offset_plus_one = ctx->program->dev.scratch_global_offset_max + 1;
       emit_load(ctx, bld, info, params);
    } else {
       info.resource = get_scratch_resource(ctx);
@@ -8050,6 +8059,7 @@
       neg_lo[0] = type_a == GLSL_TYPE_INT8;
       neg_lo[1] = type_b == GLSL_TYPE_INT8;
       break;
+   case GLSL_TYPE_FLOAT_E4M3FN: opcode = aco_opcode::v_wmma_f32_16x16x16_fp8_fp8; break;
    }
    default: unreachable("invalid cmat_muladd_amd type");
    }
@@ -8804,7 +8814,6 @@
    }
    case nir_intrinsic_terminate:
    case nir_intrinsic_terminate_if: {
-      assert(ctx->cf_info.parent_loop.exit == NULL && "Terminate must not appear in loops.");
       Operand cond = Operand::c32(-1u);
       if (instr->intrinsic == nir_intrinsic_terminate_if) {
          Temp src = get_ssa_temp(ctx, instr->src[0].ssa);
@@ -9899,24 +9908,52 @@
 void
 end_loop(isel_context* ctx, loop_context* lc)
 {
-   /* No need to check exec.potentially_empty_break/continue originating inside the loop. In the
-    * only case where it's possible at this point (divergent break after divergent continue), we
-    * should continue anyway. Terminate instructions cannot appear inside loops and demote inside
-    * divergent control flow requires WQM.
-    */
-   assert(!ctx->cf_info.exec.potentially_empty_discard);
-
-   /* Add the trivial continue. */
+   // TODO: what if a loop ends with a unconditional or uniformly branched continue
+   //       and this branch is never taken?
    if (!ctx->cf_info.has_branch) {
       unsigned loop_header_idx = ctx->cf_info.parent_loop.header_idx;
       Builder bld(ctx->program, ctx->block);
       append_logical_end(ctx->block);
 
-      ctx->block->kind |= (block_kind_continue | block_kind_uniform);
-      if (!ctx->cf_info.has_divergent_branch)
-         add_edge(ctx->block->index, &ctx->program->blocks[loop_header_idx]);
-      else
-         add_linear_edge(ctx->block->index, &ctx->program->blocks[loop_header_idx]);
+      /* No need to check exec.potentially_empty_break/continue originating inside the loop. In the
+       * only case where it's possible at this point (divergent break after divergent continue), we
+       * should continue anyway. */
+      if (ctx->cf_info.exec.potentially_empty_discard) {
+         /* Discards can result in code running with an empty exec mask.
+          * This would result in divergent breaks not ever being taken. As a
+          * workaround, break the loop when the loop mask is empty instead of
+          * always continuing. */
+         ctx->block->kind |= (block_kind_continue_or_break | block_kind_uniform);
+         unsigned block_idx = ctx->block->index;
+
+         /* create helper blocks to avoid critical edges */
+         Block* break_block = ctx->program->create_and_insert_block();
+         break_block->kind = block_kind_uniform;
+         bld.reset(break_block);
+         bld.branch(aco_opcode::p_branch);
+         add_linear_edge(block_idx, break_block);
+         add_linear_edge(break_block->index, &lc->loop_exit);
+
+         Block* continue_block = ctx->program->create_and_insert_block();
+         continue_block->kind = block_kind_uniform;
+         bld.reset(continue_block);
+         bld.branch(aco_opcode::p_branch);
+         add_linear_edge(block_idx, continue_block);
+         add_linear_edge(continue_block->index, &ctx->program->blocks[loop_header_idx]);
+
+         if (!ctx->cf_info.has_divergent_branch)
+            add_logical_edge(block_idx, &ctx->program->blocks[loop_header_idx]);
+         ctx->block = &ctx->program->blocks[block_idx];
+
+         /* SGPR temporaries might need loop exit phis to be created. */
+         ctx->program->should_repair_ssa = true;
+      } else {
+         ctx->block->kind |= (block_kind_continue | block_kind_uniform);
+         if (!ctx->cf_info.has_divergent_branch)
+            add_edge(ctx->block->index, &ctx->program->blocks[loop_header_idx]);
+         else
+            add_linear_edge(ctx->block->index, &ctx->program->blocks[loop_header_idx]);
+      }
 
       bld.reset(ctx->block);
       bld.branch(aco_opcode::p_branch);
@@ -9966,10 +10003,12 @@
       add_logical_edge(idx, logical_target);
       ctx->block->kind |= block_kind_continue;
 
-      if (!ctx->cf_info.parent_if.is_divergent) {
+      /* If exec is empty inside uniform control flow in a loop, we can assume that all invocations
+       * of the loop are inactive. Breaking from the loop is the right thing to do in that case.
+       * We shouldn't perform a uniform continue, or else we might never reach a break.
+       */
+      if (!ctx->cf_info.parent_if.is_divergent && !ctx->cf_info.exec.empty()) {
          /* uniform continue - directly jump to the loop header */
-         assert(!ctx->cf_info.exec.potentially_empty_continue &&
-                !ctx->cf_info.exec.potentially_empty_discard);
          ctx->block->kind |= block_kind_uniform;
          ctx->cf_info.has_branch = true;
          bld.branch(aco_opcode::p_branch);
@@ -9979,12 +10018,14 @@
 
       ctx->cf_info.has_divergent_branch = true;
 
-      /* for potential uniform breaks after this continue,
-         we must ensure that they are handled correctly */
-      ctx->cf_info.parent_loop.has_divergent_continue = true;
+      if (ctx->cf_info.parent_if.is_divergent) {
+         /* for potential uniform breaks after this continue,
+            we must ensure that they are handled correctly */
+         ctx->cf_info.parent_loop.has_divergent_continue = true;
 
-      if (!ctx->cf_info.exec.potentially_empty_continue)
-         ctx->cf_info.exec.potentially_empty_continue = true;
+         if (!ctx->cf_info.exec.potentially_empty_continue)
+            ctx->cf_info.exec.potentially_empty_continue = true;
+      }
    }
 
    /* remove critical edges from linear CFG */
@@ -12210,7 +12251,7 @@
 {
    uint32_t offset = 0;
 
-   assert(options->gfx_level >= GFX8 && options->gfx_level <= GFX12);
+   assert(options->gfx_level >= GFX8 && options->gfx_level <= GFX11);
 
    init_program(program, compute_cs, info, options->gfx_level, options->family, options->wgp_mode,
                 config);
@@ -12240,19 +12281,12 @@
    PhysReg ttmp2_reg{ttmp0_idx + 2};
    PhysReg ttmp3_reg{ttmp0_idx + 3};
    PhysReg tma_rsrc{ttmp0_idx + 4}; /* s4 */
-   PhysReg save_wave_status{ttmp0_idx + 8};     /* GFX8-GFX11.5 */
-   PhysReg save_wave_state_priv{ttmp0_idx + 8}; /* GFX12+ */
+   PhysReg save_wave_status{ttmp0_idx + 8};
    PhysReg save_m0{ttmp0_idx + 9};
    PhysReg save_exec{ttmp0_idx + 10}; /* s2 */
 
-   if (options->gfx_level >= GFX12) {
-      /* Save SQ_WAVE_STATE_PRIV because SCC needs to be restored. */
-      bld.sopk(aco_opcode::s_getreg_b32, Definition(save_wave_state_priv, s1),
-               ((32 - 1) << 11) | 4);
-   } else {
-      /* Save SQ_WAVE_STATUS because SCC needs to be restored. */
-      bld.sopk(aco_opcode::s_getreg_b32, Definition(save_wave_status, s1), ((32 - 1) << 11) | 2);
-   }
+   /* Save SQ_WAVE_STATUS because SCC needs to be restored. */
+   bld.sopk(aco_opcode::s_getreg_b32, Definition(save_wave_status, s1), ((32 - 1) << 11) | 2);
 
    /* Save m0. */
    bld.copy(Definition(save_m0, s1), Operand(m0, s1));
@@ -12318,60 +12352,28 @@
    }
 
    /* Store some hardware registers. */
-   if (options->gfx_level >= GFX12) {
-      const uint32_t hw_regs_idx[] = {
-         1,  /* HW_REG_MODE */
-         2,  /* HW_REG_STATUS */
-         5,  /* WH_REG_GPR_ALLOC */
-         6,  /* WH_REG_LDS_ALLOC */
-         7,  /* HW_REG_IB_STS */
-         17, /* HW_REG_EXCP_FLAG_PRIV */
-         18, /* HW_REG_EXCP_FLAG_USER */
-         19, /* HW_REG_TRAP_CTRL */
-         23, /* HW_REG_HW_ID */
-      };
-
-      offset = offsetof(struct aco_trap_handler_layout, sq_wave_regs.gfx12.state_priv);
+   const uint32_t hw_regs_idx[] = {
+      1, /* HW_REG_MODE */
+      3, /* HW_REG_TRAP_STS */
+      4, /* HW_REG_HW_ID */
+      5, /* WH_REG_GPR_ALLOC */
+      6, /* WH_REG_LDS_ALLOC */
+      7, /* HW_REG_IB_STS */
+   };
 
-      /* Store saved SQ_WAVE_STATE_PRIV which can change inside the trap. */
-      dump_sgpr_to_mem(&ctx, Operand(tma_rsrc, s4), Operand(save_wave_state_priv, s1), offset);
-      offset += 4;
+   offset = offsetof(struct aco_trap_handler_layout, sq_wave_regs.status);
 
-      for (unsigned i = 0; i < ARRAY_SIZE(hw_regs_idx); i++) {
-         /* "((size - 1) << 11) | register" */
-         bld.sopk(aco_opcode::s_getreg_b32, Definition(ttmp0_reg, s1),
-                  ((32 - 1) << 11) | hw_regs_idx[i]);
-
-         dump_sgpr_to_mem(&ctx, Operand(tma_rsrc, s4), Operand(ttmp0_reg, s1), offset);
-         offset += 4;
-      }
-   } else {
-      const uint32_t hw_regs_idx[] = {
-         1, /* HW_REG_MODE */
-         3, /* HW_REG_TRAP_STS */
-         4, /* HW_REG_HW_ID */
-         5, /* WH_REG_GPR_ALLOC */
-         6, /* WH_REG_LDS_ALLOC */
-         7, /* HW_REG_IB_STS */
-      };
+   /* Store saved SQ_WAVE_STATUS which can change inside the trap. */
+   dump_sgpr_to_mem(&ctx, Operand(tma_rsrc, s4), Operand(save_wave_status, s1), offset);
+   offset += 4;
 
-      offset = offsetof(struct aco_trap_handler_layout, sq_wave_regs.gfx8.status);
+   for (unsigned i = 0; i < ARRAY_SIZE(hw_regs_idx); i++) {
+      /* "((size - 1) << 11) | register" */
+      bld.sopk(aco_opcode::s_getreg_b32, Definition(ttmp0_reg, s1),
+               ((32 - 1) << 11) | hw_regs_idx[i]);
 
-      /* Store saved SQ_WAVE_STATUS which can change inside the trap. */
-      dump_sgpr_to_mem(&ctx, Operand(tma_rsrc, s4), Operand(save_wave_status, s1), offset);
+      dump_sgpr_to_mem(&ctx, Operand(tma_rsrc, s4), Operand(ttmp0_reg, s1), offset);
       offset += 4;
-
-      for (unsigned i = 0; i < ARRAY_SIZE(hw_regs_idx); i++) {
-         /* "((size - 1) << 11) | register" */
-         bld.sopk(aco_opcode::s_getreg_b32, Definition(ttmp0_reg, s1),
-                  ((32 - 1) << 11) | hw_regs_idx[i]);
-
-         dump_sgpr_to_mem(&ctx, Operand(tma_rsrc, s4), Operand(ttmp0_reg, s1), offset);
-         offset += 4;
-      }
-
-      /* Skip space "reserved regs". */
-      offset += 12;
    }
 
    assert(offset == offsetof(struct aco_trap_handler_layout, m0));
@@ -12402,15 +12404,9 @@
    bld.copy(Definition(m0, s1), Operand(save_m0, s1));
    bld.copy(Definition(exec, bld.lm), Operand(save_exec, bld.lm));
 
-   if (options->gfx_level >= GFX12) {
-      /* Restore SCC which is the bit 9 of SQ_WAVE_STATE_PRIV. */
-      bld.sopc(aco_opcode::s_bitcmp1_b32, bld.def(s1, scc), Operand(save_wave_state_priv, s1),
-               Operand::c32(9u));
-   } else {
-      /* Restore SCC which is the first bit of SQ_WAVE_STATUS. */
-      bld.sopc(aco_opcode::s_bitcmp1_b32, bld.def(s1, scc), Operand(save_wave_status, s1),
-               Operand::c32(0u));
-   }
+   /* Restore SCC which is the first bit of SQ_WAVE_STATUS. */
+   bld.sopc(aco_opcode::s_bitcmp1_b32, bld.def(s1, scc), Operand(save_wave_status, s1),
+            Operand::c32(0u));
 
    program->config->float_mode = program->blocks[0].fp_mode.val;
 
@@ -12665,16 +12661,10 @@
             Operand(tmp_raygen_sbt, s2), Operand::c32(0u));
 
    /* load ray launch sizes */
-   assert(out_launch_size_x.reg() % 4 == 0);
-   if (options->gfx_level >= GFX12) {
-      bld.smem(aco_opcode::s_load_dwordx3, Definition(out_launch_size_x, s3),
-               Operand(in_launch_size_addr, s2), Operand::c32(0u));
-   } else {
-      bld.smem(aco_opcode::s_load_dword, Definition(out_launch_size_z, s1),
-               Operand(in_launch_size_addr, s2), Operand::c32(8u));
-      bld.smem(aco_opcode::s_load_dwordx2, Definition(out_launch_size_x, s2),
-               Operand(in_launch_size_addr, s2), Operand::c32(0u));
-   }
+   bld.smem(aco_opcode::s_load_dword, Definition(out_launch_size_z, s1),
+            Operand(in_launch_size_addr, s2), Operand::c32(8u));
+   bld.smem(aco_opcode::s_load_dwordx2, Definition(out_launch_size_x, s2),
+            Operand(in_launch_size_addr, s2), Operand::c32(0u));
 
    /* calculate ray launch ids */
    if (options->gfx_level >= GFX11) {
--- mesa-chaotic/src/amd/compiler/aco_instruction_selection_setup.cpp	2025-05-11 17:14:25.514443603 +0200
+++ mesa-dadschoorse/src/amd/compiler/aco_instruction_selection_setup.cpp	2025-05-11 17:09:56.092770741 +0200
@@ -413,6 +413,8 @@
                       regclasses[alu_instr->src[0].src.ssa->index].type() == RegType::vgpr)
                      type = RegType::vgpr;
                   break;
+               case nir_op_f2e4m3fn:
+               case nir_op_e4m3fn2f:
                case nir_op_fmulz:
                case nir_op_ffmaz:
                case nir_op_f2f64:
--- mesa-chaotic/src/amd/compiler/aco_interface.cpp	2025-05-11 17:14:25.514443603 +0200
+++ mesa-dadschoorse/src/amd/compiler/aco_interface.cpp	2025-05-11 17:09:56.092770741 +0200
@@ -446,8 +446,9 @@
 {
    init();
    /* Exclude flags which don't affect code generation. */
-   uint64_t exclude = DEBUG_VALIDATE_IR | DEBUG_VALIDATE_RA | DEBUG_PERF_INFO | DEBUG_LIVE_INFO |
-                      DEBUG_NO_VALIDATE | DEBUG_VALIDATE_LIVE_VARS | DEBUG_VALIDATE_OPT;
+   uint64_t exclude =
+      DEBUG_VALIDATE_IR | DEBUG_VALIDATE_RA | DEBUG_PERF_INFO | DEBUG_LIVE_INFO |
+      DEBUG_NO_VALIDATE_IR | DEBUG_VALIDATE_LIVE_VARS;
    return debug_flags & ~exclude;
 }
 
@@ -482,6 +483,8 @@
       return (shader->options->force_f2f16_rtz && !nir_is_rounding_mode_rtne(execution_mode, 16)) ||
              nir_is_rounding_mode_rtz(execution_mode, 16);
    }
+   case nir_op_f2e4m3fn:
+   case nir_op_e4m3fn2f:
    case nir_op_fadd:
    case nir_op_fsub:
    case nir_op_fmul:
--- mesa-chaotic/src/amd/compiler/aco_ir.cpp	2025-05-11 17:14:25.514443603 +0200
+++ mesa-dadschoorse/src/amd/compiler/aco_ir.cpp	2025-05-11 17:09:56.092770741 +0200
@@ -22,8 +22,7 @@
    {"validateir", DEBUG_VALIDATE_IR},
    {"validatera", DEBUG_VALIDATE_RA},
    {"validate-livevars", DEBUG_VALIDATE_LIVE_VARS},
-   {"validateopt", DEBUG_VALIDATE_OPT},
-   {"novalidate", DEBUG_NO_VALIDATE},
+   {"novalidateir", DEBUG_NO_VALIDATE_IR},
    {"force-waitcnt", DEBUG_FORCE_WAITCNT},
    {"force-waitdeps", DEBUG_FORCE_WAITDEPS},
    {"novn", DEBUG_NO_VN},
@@ -44,10 +43,11 @@
 
 #ifndef NDEBUG
    /* enable some flags by default on debug builds */
-   if (!(debug_flags & aco::DEBUG_NO_VALIDATE)) {
-      debug_flags |= aco::DEBUG_VALIDATE_IR | DEBUG_VALIDATE_OPT;
-   }
+   debug_flags |= aco::DEBUG_VALIDATE_IR;
 #endif
+
+   if (debug_flags & aco::DEBUG_NO_VALIDATE_IR)
+      debug_flags &= ~aco::DEBUG_VALIDATE_IR;
 }
 
 void
@@ -585,6 +585,8 @@
    case aco_opcode::v_interp_p10_rtz_f16_f32_inreg: return idx == 0 || idx == 2;
    case aco_opcode::v_interp_p2_f16_f32_inreg:
    case aco_opcode::v_interp_p2_rtz_f16_f32_inreg: return idx == -1 || idx == 0;
+   case aco_opcode::v_cvt_pk_fp8_f32:
+   case aco_opcode::v_cvt_pk_bf8_f32: return idx == -1;
    default:
       return gfx_level >= GFX11 && (get_gfx11_true16_mask(op) & BITFIELD_BIT(idx == -1 ? 3 : idx));
    }
@@ -716,6 +718,8 @@
    case aco_opcode::v_and_b16:
    case aco_opcode::v_or_b16:
    case aco_opcode::v_xor_b16: return 0x3 | 0x8;
+   case aco_opcode::v_cvt_pk_f32_fp8:
+   case aco_opcode::v_cvt_pk_f32_bf8:
    case aco_opcode::v_cvt_f32_f16:
    case aco_opcode::v_cvt_i32_i16:
    case aco_opcode::v_cvt_u32_u16: return 0x1;
--- mesa-chaotic/src/amd/compiler/aco_optimizer.cpp	2025-05-11 17:14:25.516064171 +0200
+++ mesa-dadschoorse/src/amd/compiler/aco_optimizer.cpp	2025-05-11 17:09:56.092770741 +0200
@@ -43,12 +43,14 @@
 };
 
 enum Label {
+   label_vec = 1 << 0,
    label_constant_32bit = 1 << 1,
    /* label_{abs,neg,mul,omod2,omod4,omod5,clamp} are used for both 16 and
     * 32-bit operations but this shouldn't cause any issues because we don't
     * look through any conversions */
    label_abs = 1 << 2,
    label_neg = 1 << 3,
+   label_mul = 1 << 4,
    label_temp = 1 << 5,
    label_literal = 1 << 6,
    label_mad = 1 << 7,
@@ -57,6 +59,10 @@
    label_omod5 = 1 << 10,
    label_clamp = 1 << 12,
    label_b2f = 1 << 16,
+   label_add_sub = 1 << 17,
+   label_bitwise = 1 << 18,
+   label_minmax = 1 << 19,
+   label_vopc = 1 << 20,
    label_uniform_bool = 1 << 21,
    label_constant_64bit = 1 << 22,
    label_uniform_bitwise = 1 << 23,
@@ -65,23 +71,34 @@
    label_b2i = 1 << 27,
    label_fcanonicalize = 1 << 28,
    label_constant_16bit = 1 << 29,
-   label_canonicalized = 1ull << 32, /* 1ull to prevent sign extension */
+   label_usedef = 1 << 30,   /* generic label */
+   label_vop3p = 1ull << 31, /* 1ull to prevent sign extension */
+   label_canonicalized = 1ull << 32,
    label_extract = 1ull << 33,
    label_insert = 1ull << 34,
+   label_dpp16 = 1ull << 35,
+   label_dpp8 = 1ull << 36,
+   label_f2f32 = 1ull << 37,
    label_f2f16 = 1ull << 38,
+   label_split = 1ull << 39,
 };
 
+static constexpr uint64_t instr_usedef_labels =
+   label_vec | label_mul | label_add_sub | label_vop3p | label_bitwise | label_uniform_bitwise |
+   label_minmax | label_vopc | label_usedef | label_extract | label_dpp16 | label_dpp8 |
+   label_f2f32;
 static constexpr uint64_t instr_mod_labels =
    label_omod2 | label_omod4 | label_omod5 | label_clamp | label_insert | label_f2f16;
 
+static constexpr uint64_t instr_labels = instr_usedef_labels | instr_mod_labels | label_split;
 static constexpr uint64_t temp_labels = label_abs | label_neg | label_temp | label_b2f |
                                         label_uniform_bool | label_scc_invert | label_b2i |
                                         label_fcanonicalize;
 static constexpr uint32_t val_labels =
    label_constant_32bit | label_constant_64bit | label_constant_16bit | label_literal | label_mad;
 
-static_assert((instr_mod_labels & temp_labels) == 0, "labels cannot intersect");
-static_assert((instr_mod_labels & val_labels) == 0, "labels cannot intersect");
+static_assert((instr_labels & temp_labels) == 0, "labels cannot intersect");
+static_assert((instr_labels & val_labels) == 0, "labels cannot intersect");
 static_assert((temp_labels & val_labels) == 0, "labels cannot intersect");
 
 struct ssa_info {
@@ -89,37 +106,50 @@
    union {
       uint32_t val;
       Temp temp;
-      Instruction* mod_instr;
+      Instruction* instr;
    };
-   Instruction* parent_instr;
 
    ssa_info() : label(0) {}
 
    void add_label(Label new_label)
    {
+      /* Since all the instr_usedef_labels use instr for the same thing
+       * (indicating the defining instruction), there is usually no need to
+       * clear any other instr labels. */
+      if (new_label & instr_usedef_labels)
+         label &= ~(instr_mod_labels | temp_labels | val_labels); /* instr, temp and val alias */
+
       if (new_label & instr_mod_labels) {
-         label &= ~instr_mod_labels;
+         label &= ~instr_labels;
          label &= ~(temp_labels | val_labels); /* instr, temp and val alias */
       }
 
       if (new_label & temp_labels) {
          label &= ~temp_labels;
-         label &= ~(instr_mod_labels | val_labels); /* instr, temp and val alias */
+         label &= ~(instr_labels | val_labels); /* instr, temp and val alias */
       }
 
       uint32_t const_labels =
          label_literal | label_constant_32bit | label_constant_64bit | label_constant_16bit;
       if (new_label & const_labels) {
          label &= ~val_labels | const_labels;
-         label &= ~(instr_mod_labels | temp_labels); /* instr, temp and val alias */
+         label &= ~(instr_labels | temp_labels); /* instr, temp and val alias */
       } else if (new_label & val_labels) {
          label &= ~val_labels;
-         label &= ~(instr_mod_labels | temp_labels); /* instr, temp and val alias */
+         label &= ~(instr_labels | temp_labels); /* instr, temp and val alias */
       }
 
       label |= new_label;
    }
 
+   void set_vec(Instruction* vec)
+   {
+      add_label(label_vec);
+      instr = vec;
+   }
+
+   bool is_vec() { return label & label_vec; }
+
    void set_constant(amd_gfx_level gfx_level, uint64_t constant)
    {
       Operand op16 = Operand::c16(constant);
@@ -198,6 +228,14 @@
       temp = neg_abs_temp;
    }
 
+   void set_mul(Instruction* mul)
+   {
+      add_label(label_mul);
+      instr = mul;
+   }
+
+   bool is_mul() { return label & label_mul; }
+
    void set_temp(Temp tmp)
    {
       add_label(label_temp);
@@ -219,7 +257,7 @@
       if (label & temp_labels)
          return;
       add_label(label_omod2);
-      mod_instr = mul;
+      instr = mul;
    }
 
    bool is_omod2() { return label & label_omod2; }
@@ -229,7 +267,7 @@
       if (label & temp_labels)
          return;
       add_label(label_omod4);
-      mod_instr = mul;
+      instr = mul;
    }
 
    bool is_omod4() { return label & label_omod4; }
@@ -239,7 +277,7 @@
       if (label & temp_labels)
          return;
       add_label(label_omod5);
-      mod_instr = mul;
+      instr = mul;
    }
 
    bool is_omod5() { return label & label_omod5; }
@@ -249,7 +287,7 @@
       if (label & temp_labels)
          return;
       add_label(label_clamp);
-      mod_instr = med3;
+      instr = med3;
    }
 
    bool is_clamp() { return label & label_clamp; }
@@ -259,7 +297,7 @@
       if (label & temp_labels)
          return;
       add_label(label_f2f16);
-      mod_instr = conv;
+      instr = conv;
    }
 
    bool is_f2f16() { return label & label_f2f16; }
@@ -272,10 +310,42 @@
 
    bool is_b2f() { return label & label_b2f; }
 
+   void set_add_sub(Instruction* add_sub_instr)
+   {
+      add_label(label_add_sub);
+      instr = add_sub_instr;
+   }
+
+   bool is_add_sub() { return label & label_add_sub; }
+
+   void set_bitwise(Instruction* bitwise_instr)
+   {
+      add_label(label_bitwise);
+      instr = bitwise_instr;
+   }
+
+   bool is_bitwise() { return label & label_bitwise; }
+
    void set_uniform_bitwise() { add_label(label_uniform_bitwise); }
 
    bool is_uniform_bitwise() { return label & label_uniform_bitwise; }
 
+   void set_minmax(Instruction* minmax_instr)
+   {
+      add_label(label_minmax);
+      instr = minmax_instr;
+   }
+
+   bool is_minmax() { return label & label_minmax; }
+
+   void set_vopc(Instruction* vopc_instr)
+   {
+      add_label(label_vopc);
+      instr = vopc_instr;
+   }
+
+   bool is_vopc() { return label & label_vopc; }
+
    void set_scc_needed() { add_label(label_scc_needed); }
 
    bool is_scc_needed() { return label & label_scc_needed; }
@@ -304,6 +374,22 @@
 
    bool is_b2i() { return label & label_b2i; }
 
+   void set_usedef(Instruction* label_instr)
+   {
+      add_label(label_usedef);
+      instr = label_instr;
+   }
+
+   bool is_usedef() { return label & label_usedef; }
+
+   void set_vop3p(Instruction* vop3p_instr)
+   {
+      add_label(label_vop3p);
+      instr = vop3p_instr;
+   }
+
+   bool is_vop3p() { return label & label_vop3p; }
+
    void set_fcanonicalize(Temp tmp)
    {
       add_label(label_fcanonicalize);
@@ -316,7 +402,19 @@
 
    bool is_canonicalized() { return label & label_canonicalized; }
 
-   void set_extract() { add_label(label_extract); }
+   void set_f2f32(Instruction* cvt)
+   {
+      add_label(label_f2f32);
+      instr = cvt;
+   }
+
+   bool is_f2f32() { return label & label_f2f32; }
+
+   void set_extract(Instruction* extract)
+   {
+      add_label(label_extract);
+      instr = extract;
+   }
 
    bool is_extract() { return label & label_extract; }
 
@@ -325,10 +423,34 @@
       if (label & temp_labels)
          return;
       add_label(label_insert);
-      mod_instr = insert;
+      instr = insert;
    }
 
    bool is_insert() { return label & label_insert; }
+
+   void set_dpp16(Instruction* mov)
+   {
+      add_label(label_dpp16);
+      instr = mov;
+   }
+
+   void set_dpp8(Instruction* mov)
+   {
+      add_label(label_dpp8);
+      instr = mov;
+   }
+
+   bool is_dpp() { return label & (label_dpp16 | label_dpp8); }
+   bool is_dpp16() { return label & label_dpp16; }
+   bool is_dpp8() { return label & label_dpp8; }
+
+   void set_split(Instruction* split)
+   {
+      add_label(label_split);
+      instr = split;
+   }
+
+   bool is_split() { return label & label_split; }
 };
 
 struct opt_ctx {
@@ -467,7 +589,8 @@
           instr->opcode != aco_opcode::v_wmma_f16_16x16x16_f16 &&
           instr->opcode != aco_opcode::v_wmma_bf16_16x16x16_bf16 &&
           instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu8 &&
-          instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu4;
+          instr->opcode != aco_opcode::v_wmma_i32_16x16x16_iu4 &&
+          instr->opcode != aco_opcode::v_wmma_f32_16x16x16_fp8_fp8;
 }
 
 /* only covers special cases */
@@ -528,6 +651,7 @@
    case aco_opcode::v_interp_p2_rtz_f16_f32_inreg:
    case aco_opcode::v_dot2_bf16_bf16: /* TODO */
    case aco_opcode::v_wmma_f32_16x16x16_f16:
+   case aco_opcode::v_wmma_f32_16x16x16_fp8_fp8:
    case aco_opcode::v_wmma_f32_16x16x16_bf16:
    case aco_opcode::v_wmma_f16_16x16x16_f16:
    case aco_opcode::v_wmma_bf16_16x16x16_bf16:
@@ -609,12 +733,11 @@
    if (!op.isTemp())
       return false;
    Temp tmp = op.getTemp();
-
-   Instruction* add_instr = ctx.info[tmp.id()].parent_instr;
-
-   if (add_instr->definitions[0].getTemp() != tmp)
+   if (!ctx.info[tmp.id()].is_add_sub())
       return false;
 
+   Instruction* add_instr = ctx.info[tmp.id()].instr;
+
    unsigned mask = 0x3;
    bool is_sub = false;
    switch (add_instr->opcode) {
@@ -671,7 +794,7 @@
 }
 
 void
-skip_smem_offset_align(opt_ctx& ctx, SMEM_instruction* smem, uint32_t align)
+skip_smem_offset_align(opt_ctx& ctx, SMEM_instruction* smem)
 {
    bool soe = smem->operands.size() >= (!smem->definitions.empty() ? 3 : 4);
    if (soe && !smem->operands[1].isConstant())
@@ -681,19 +804,17 @@
     */
 
    Operand& op = smem->operands[soe ? smem->operands.size() - 1 : 1];
-   if (!op.isTemp())
+   if (!op.isTemp() || !ctx.info[op.tempId()].is_bitwise())
       return;
 
-   Instruction* bitwise_instr = ctx.info[op.tempId()].parent_instr;
-   if (bitwise_instr->opcode != aco_opcode::s_and_b32 ||
-       bitwise_instr->definitions[0].getTemp() != op.getTemp())
+   Instruction* bitwise_instr = ctx.info[op.tempId()].instr;
+   if (bitwise_instr->opcode != aco_opcode::s_and_b32)
       return;
 
-   uint32_t mask = ~(align - 1u);
-   if (bitwise_instr->operands[0].constantEquals(mask) &&
+   if (bitwise_instr->operands[0].constantEquals(-4) &&
        bitwise_instr->operands[1].isOfType(op.regClass().type()))
       op.setTemp(bitwise_instr->operands[1].getTemp());
-   else if (bitwise_instr->operands[1].constantEquals(mask) &&
+   else if (bitwise_instr->operands[1].constantEquals(-4) &&
             bitwise_instr->operands[0].isOfType(op.regClass().type()))
       op.setTemp(bitwise_instr->operands[0].getTemp());
 }
@@ -701,22 +822,9 @@
 void
 smem_combine(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
-   uint32_t align = 4;
-   switch (instr->opcode) {
-   case aco_opcode::s_load_sbyte:
-   case aco_opcode::s_load_ubyte:
-   case aco_opcode::s_buffer_load_sbyte:
-   case aco_opcode::s_buffer_load_ubyte: align = 1; break;
-   case aco_opcode::s_load_sshort:
-   case aco_opcode::s_load_ushort:
-   case aco_opcode::s_buffer_load_sshort:
-   case aco_opcode::s_buffer_load_ushort: align = 2; break;
-   default: break;
-   }
-
    /* skip &-4 before offset additions: load((a + 16) & -4, 0) */
-   if (!instr->operands.empty() && align > 1)
-      skip_smem_offset_align(ctx, &instr->smem(), align);
+   if (!instr->operands.empty())
+      skip_smem_offset_align(ctx, &instr->smem());
 
    /* propagate constants and combine additions */
    if (!instr->operands.empty() && instr->operands[1].isTemp()) {
@@ -729,7 +837,7 @@
          instr->operands[1] = Operand::c32(info.val);
       } else if (parse_base_offset(ctx, instr.get(), 1, &base, &offset, true) &&
                  base.regClass() == s1 && offset <= ctx.program->dev.smem_offset_max &&
-                 ctx.program->gfx_level >= GFX9 && offset % align == 0) {
+                 ctx.program->gfx_level >= GFX9 && offset % 4u == 0) {
          bool soe = smem.operands.size() >= (!smem.definitions.empty() ? 3 : 4);
          if (soe) {
             if (ctx.info[smem.operands.back().tempId()].is_constant_or_literal(32) &&
@@ -755,8 +863,8 @@
    }
 
    /* skip &-4 after offset additions: load(a & -4, 16) */
-   if (!instr->operands.empty() && align > 1)
-      skip_smem_offset_align(ctx, &instr->smem(), align);
+   if (!instr->operands.empty())
+      skip_smem_offset_align(ctx, &instr->smem());
 }
 
 Operand
@@ -933,8 +1041,8 @@
 bool
 can_apply_extract(opt_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, ssa_info& info)
 {
-   Temp tmp = info.parent_instr->operands[0].getTemp();
-   SubdwordSel sel = parse_extract(info.parent_instr);
+   Temp tmp = info.instr->operands[0].getTemp();
+   SubdwordSel sel = parse_extract(info.instr);
 
    if (!sel) {
       return false;
@@ -979,8 +1087,7 @@
       return true;
    } else if (instr->opcode == aco_opcode::p_extract ||
               instr->opcode == aco_opcode::p_extract_vector) {
-      if (ctx.program->gfx_level < GFX9 &&
-          !info.parent_instr->operands[0].isOfType(RegType::vgpr) &&
+      if (ctx.program->gfx_level < GFX9 && !info.instr->operands[0].isOfType(RegType::vgpr) &&
           instr->definitions[0].regClass().is_subdword())
          return false;
 
@@ -998,8 +1105,8 @@
 void
 apply_extract(opt_ctx& ctx, aco_ptr<Instruction>& instr, unsigned idx, ssa_info& info)
 {
-   Temp tmp = info.parent_instr->operands[0].getTemp();
-   SubdwordSel sel = parse_extract(info.parent_instr);
+   Temp tmp = info.instr->operands[0].getTemp();
+   SubdwordSel sel = parse_extract(info.instr);
    assert(sel);
 
    instr->operands[idx].set16bit(false);
@@ -1025,7 +1132,7 @@
               ((sel.size() == 2 && instr->operands[0].constantValue() >= 16u) ||
                (sel.size() == 1 && instr->operands[0].constantValue() >= 24u))) {
       /* The undesirable upper bits are already shifted out. */
-      if (!instr->isVOP3() && !info.parent_instr->operands[0].isOfType(RegType::vgpr))
+      if (!instr->isVOP3() && !info.instr->operands[0].isOfType(RegType::vgpr))
          instr->format = asVOP3(instr->format);
       return;
    } else if (instr->opcode == aco_opcode::v_mul_u32_u24 && ctx.program->gfx_level >= GFX10 &&
@@ -1055,7 +1162,7 @@
 
          /* VOP12C cannot use opsel with SGPRs. */
          if (!instr->isVOP3() && !instr->isVINTERP_INREG() &&
-             !info.parent_instr->operands[0].isOfType(RegType::vgpr))
+             !info.instr->operands[0].isOfType(RegType::vgpr))
             instr->format = asVOP3(instr->format);
       }
    } else if (instr->opcode == aco_opcode::s_pack_ll_b32_b16) {
@@ -1102,8 +1209,10 @@
 
    /* These are the only labels worth keeping at the moment. */
    for (Definition& def : instr->definitions) {
-      ctx.info[def.tempId()].label &= instr_mod_labels;
-      ctx.info[def.tempId()].parent_instr = instr.get();
+      ctx.info[def.tempId()].label &=
+         (label_mul | label_minmax | label_usedef | label_vopc | label_f2f32 | instr_mod_labels);
+      if (ctx.info[def.tempId()].label & instr_usedef_labels)
+         ctx.info[def.tempId()].instr = instr.get();
    }
 }
 
@@ -1115,7 +1224,7 @@
       if (!op.isTemp())
          continue;
       ssa_info& info = ctx.info[op.tempId()];
-      if (info.is_extract() && (info.parent_instr->operands[0].getTemp().type() == RegType::vgpr ||
+      if (info.is_extract() && (info.instr->operands[0].getTemp().type() == RegType::vgpr ||
                                 op.getTemp().type() == RegType::sgpr)) {
          if (!can_apply_extract(ctx, instr, i, info))
             info.label &= ~label_extract;
@@ -1158,30 +1267,27 @@
 bool
 can_eliminate_and_exec(opt_ctx& ctx, Temp tmp, unsigned pass_flags)
 {
-   Instruction* instr = ctx.info[tmp.id()].parent_instr;
-   /* Remove superfluous s_and when the VOPC instruction uses the same exec and thus
-    * already produces the same result */
-   if (instr->isVOPC())
-      return instr->pass_flags == pass_flags;
-
-   if (instr->operands.size() != 2 || instr->pass_flags != pass_flags)
-      return false;
-   if (!(instr->operands[0].isTemp() && instr->operands[1].isTemp()))
-      return false;
-
-   switch (instr->opcode) {
-   case aco_opcode::s_and_b32:
-   case aco_opcode::s_and_b64:
-      return can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), pass_flags) ||
-             can_eliminate_and_exec(ctx, instr->operands[1].getTemp(), pass_flags);
-   case aco_opcode::s_or_b32:
-   case aco_opcode::s_or_b64:
-   case aco_opcode::s_xor_b32:
-   case aco_opcode::s_xor_b64:
-      return can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), pass_flags) &&
-             can_eliminate_and_exec(ctx, instr->operands[1].getTemp(), pass_flags);
-   default: return false;
+   if (ctx.info[tmp.id()].is_vopc()) {
+      Instruction* vopc_instr = ctx.info[tmp.id()].instr;
+      /* Remove superfluous s_and when the VOPC instruction uses the same exec and thus
+       * already produces the same result */
+      return vopc_instr->pass_flags == pass_flags;
+   }
+   if (ctx.info[tmp.id()].is_bitwise()) {
+      Instruction* instr = ctx.info[tmp.id()].instr;
+      if (instr->operands.size() != 2 || instr->pass_flags != pass_flags)
+         return false;
+      if (!(instr->operands[0].isTemp() && instr->operands[1].isTemp()))
+         return false;
+      if (instr->opcode == aco_opcode::s_and_b32 || instr->opcode == aco_opcode::s_and_b64) {
+         return can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), pass_flags) ||
+                can_eliminate_and_exec(ctx, instr->operands[1].getTemp(), pass_flags);
+      } else {
+         return can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), pass_flags) &&
+                can_eliminate_and_exec(ctx, instr->operands[1].getTemp(), pass_flags);
+      }
    }
+   return false;
 }
 
 bool
@@ -1398,14 +1504,12 @@
 
          uint32_t const_max = ctx.program->dev.buf_offset_max;
 
-         if (mubuf.offen && mubuf.idxen && i == 1 &&
-             info.parent_instr->opcode == aco_opcode::p_create_vector &&
-             info.parent_instr->operands.size() == 2 && info.parent_instr->operands[0].isTemp() &&
-             info.parent_instr->operands[0].regClass() == v1 &&
-             info.parent_instr->operands[1].isConstant() &&
-             mubuf.offset + info.parent_instr->operands[1].constantValue() <= const_max) {
-            instr->operands[1] = info.parent_instr->operands[0];
-            mubuf.offset += info.parent_instr->operands[1].constantValue();
+         if (mubuf.offen && mubuf.idxen && i == 1 && info.is_vec() &&
+             info.instr->operands.size() == 2 && info.instr->operands[0].isTemp() &&
+             info.instr->operands[0].regClass() == v1 && info.instr->operands[1].isConstant() &&
+             mubuf.offset + info.instr->operands[1].constantValue() <= const_max) {
+            instr->operands[1] = info.instr->operands[0];
+            mubuf.offset += info.instr->operands[1].constantValue();
             mubuf.offen = false;
             continue;
          } else if (mubuf.offen && i == 1 && info.is_constant_or_literal(32) &&
@@ -1441,15 +1545,13 @@
          while (info.is_temp())
             info = ctx.info[info.temp.id()];
 
-         if (mtbuf.offen && mtbuf.idxen && i == 1 &&
-             info.parent_instr->opcode == aco_opcode::p_create_vector &&
-             info.parent_instr->operands.size() == 2 && info.parent_instr->operands[0].isTemp() &&
-             info.parent_instr->operands[0].regClass() == v1 &&
-             info.parent_instr->operands[1].isConstant() &&
-             mtbuf.offset + info.parent_instr->operands[1].constantValue() <=
+         if (mtbuf.offen && mtbuf.idxen && i == 1 && info.is_vec() &&
+             info.instr->operands.size() == 2 && info.instr->operands[0].isTemp() &&
+             info.instr->operands[0].regClass() == v1 && info.instr->operands[1].isConstant() &&
+             mtbuf.offset + info.instr->operands[1].constantValue() <=
                 ctx.program->dev.buf_offset_max) {
-            instr->operands[1] = info.parent_instr->operands[0];
-            mtbuf.offset += info.parent_instr->operands[1].constantValue();
+            instr->operands[1] = info.instr->operands[0];
+            mtbuf.offset += info.instr->operands[1].constantValue();
             mtbuf.offen = false;
             continue;
          }
@@ -1565,6 +1667,16 @@
          if (canonicalized)
             ctx.info[instr->definitions[0].tempId()].set_canonicalized();
       }
+
+      if (instr->isVOPC()) {
+         ctx.info[instr->definitions[0].tempId()].set_vopc(instr.get());
+         check_sdwa_extract(ctx, instr);
+         return;
+      }
+      if (instr->isVOP3P()) {
+         ctx.info[instr->definitions[0].tempId()].set_vop3p(instr.get());
+         return;
+      }
    }
 
    switch (instr->opcode) {
@@ -1583,9 +1695,8 @@
          /* ensure that any expanded operands are properly aligned */
          bool aligned = offset % 4 == 0 || op.bytes() < 4;
          offset += op.bytes();
-         if (aligned && op.isTemp() &&
-             ctx.info[op.tempId()].parent_instr->opcode == aco_opcode::p_create_vector) {
-            Instruction* vec = ctx.info[op.tempId()].parent_instr;
+         if (aligned && op.isTemp() && ctx.info[op.tempId()].is_vec()) {
+            Instruction* vec = ctx.info[op.tempId()].instr;
             for (const Operand& vec_op : vec->operands)
                ops.emplace_back(vec_op);
          } else {
@@ -1611,16 +1722,14 @@
             assert(instr->operands[i] == ops[i]);
          }
       }
+      ctx.info[instr->definitions[0].tempId()].set_vec(instr.get());
 
-      if (instr->operands.size() == 2 && instr->operands[1].isTemp()) {
+      if (instr->operands.size() == 2) {
          /* check if this is created from split_vector */
-         ssa_info& info = ctx.info[instr->operands[1].tempId()];
-         if (info.parent_instr->opcode == aco_opcode::p_split_vector) {
-            Instruction* split = info.parent_instr;
+         if (instr->operands[1].isTemp() && ctx.info[instr->operands[1].tempId()].is_split()) {
+            Instruction* split = ctx.info[instr->operands[1].tempId()].instr;
             if (instr->operands[0].isTemp() &&
-                instr->operands[0].getTemp() == split->definitions[0].getTemp() &&
-                instr->operands[1].getTemp() == split->definitions[1].getTemp() &&
-                instr->definitions[0].regClass() == split->operands[0].regClass())
+                instr->operands[0].getTemp() == split->definitions[0].getTemp())
                ctx.info[instr->definitions[0].tempId()].set_temp(split->operands[0].getTemp());
          }
       }
@@ -1637,19 +1746,20 @@
             val >>= def.bytes() * 8u;
          }
          break;
-      } else if (info.parent_instr->opcode != aco_opcode::p_create_vector) {
+      } else if (!info.is_vec()) {
          if (instr->definitions.size() == 2 && instr->operands[0].isTemp() &&
              instr->definitions[0].bytes() == instr->definitions[1].bytes()) {
+            ctx.info[instr->definitions[1].tempId()].set_split(instr.get());
             if (instr->operands[0].bytes() == 4) {
                /* D16 subdword split */
                ctx.info[instr->definitions[0].tempId()].set_temp(instr->operands[0].getTemp());
-               ctx.info[instr->definitions[1].tempId()].set_extract();
+               ctx.info[instr->definitions[1].tempId()].set_extract(instr.get());
             }
          }
          break;
       }
 
-      Instruction* vec = info.parent_instr;
+      Instruction* vec = ctx.info[instr->operands[0].tempId()].instr;
       unsigned split_offset = 0;
       unsigned vec_offset = 0;
       unsigned vec_index = 0;
@@ -1679,9 +1789,9 @@
          ssa_info& info = ctx.info[instr->operands[0].tempId()];
          const unsigned dst_offset = index * instr->definitions[0].bytes();
 
-         if (info.parent_instr->opcode == aco_opcode::p_create_vector) {
+         if (info.is_vec()) {
             /* check if we index directly into a vector element */
-            Instruction* vec = info.parent_instr;
+            Instruction* vec = info.instr;
             unsigned offset = 0;
 
             for (const Operand& op : vec->operands) {
@@ -1711,7 +1821,7 @@
          if (index == 0)
             ctx.info[instr->definitions[0].tempId()].set_temp(instr->operands[0].getTemp());
          else
-            ctx.info[instr->definitions[0].tempId()].set_extract();
+            ctx.info[instr->definitions[0].tempId()].set_extract(instr.get());
          break;
       }
 
@@ -1721,14 +1831,12 @@
       FALLTHROUGH;
    }
    case aco_opcode::p_parallelcopy: /* propagate */
-      if (instr->operands[0].isTemp() &&
-          ctx.info[instr->operands[0].tempId()].parent_instr->opcode ==
-             aco_opcode::p_create_vector &&
+      if (instr->operands[0].isTemp() && ctx.info[instr->operands[0].tempId()].is_vec() &&
           instr->operands[0].regClass() != instr->definitions[0].regClass()) {
          /* We might not be able to copy-propagate if it's a SGPR->VGPR copy, so
           * duplicate the vector instead.
           */
-         Instruction* vec = ctx.info[instr->operands[0].tempId()].parent_instr;
+         Instruction* vec = ctx.info[instr->operands[0].tempId()].instr;
          aco_ptr<Instruction> old_copy = std::move(instr);
 
          instr.reset(create_instruction(aco_opcode::p_create_vector, Format::PSEUDO,
@@ -1741,6 +1849,7 @@
                 ctx.info[op.tempId()].temp.type() == instr->definitions[0].regClass().type())
                op.setTemp(ctx.info[op.tempId()].temp);
          }
+         ctx.info[instr->definitions[0].tempId()].set_vec(instr.get());
          break;
       }
       FALLTHROUGH;
@@ -1758,13 +1867,26 @@
          assert(instr->operands[0].isFixed());
       }
       break;
+   case aco_opcode::v_mov_b32:
+      if (instr->isDPP16()) {
+         /* anything else doesn't make sense in SSA */
+         assert(instr->dpp16().row_mask == 0xf && instr->dpp16().bank_mask == 0xf);
+         ctx.info[instr->definitions[0].tempId()].set_dpp16(instr.get());
+      } else if (instr->isDPP8()) {
+         ctx.info[instr->definitions[0].tempId()].set_dpp8(instr.get());
+      }
+      break;
    case aco_opcode::p_is_helper:
       if (!ctx.program->needs_wqm)
          ctx.info[instr->definitions[0].tempId()].set_constant(ctx.program->gfx_level, 0u);
       break;
+   case aco_opcode::v_mul_f64_e64:
+   case aco_opcode::v_mul_f64: ctx.info[instr->definitions[0].tempId()].set_mul(instr.get()); break;
    case aco_opcode::v_mul_f16:
    case aco_opcode::v_mul_f32:
    case aco_opcode::v_mul_legacy_f32: { /* omod */
+      ctx.info[instr->definitions[0].tempId()].set_mul(instr.get());
+
       /* TODO: try to move the negate/abs modifier to the consumer instead */
       bool uses_mods = instr->usesModifiers();
       bool fp16 = instr->opcode == aco_opcode::v_mul_f16;
@@ -1831,6 +1953,11 @@
       }
       break;
    }
+   case aco_opcode::v_mul_lo_u16:
+   case aco_opcode::v_mul_lo_u16_e64:
+   case aco_opcode::v_mul_u32_u24:
+      ctx.info[instr->definitions[0].tempId()].set_usedef(instr.get());
+      break;
    case aco_opcode::v_med3_f16:
    case aco_opcode::v_med3_f32: { /* clamp */
       unsigned idx;
@@ -1845,6 +1972,23 @@
          ctx.info[instr->definitions[0].tempId()].set_b2i(instr->operands[2].getTemp());
 
       break;
+   case aco_opcode::v_add_u32:
+   case aco_opcode::v_add_co_u32:
+   case aco_opcode::v_add_co_u32_e64:
+   case aco_opcode::s_add_i32:
+   case aco_opcode::s_add_u32:
+   case aco_opcode::v_subbrev_co_u32:
+   case aco_opcode::v_sub_u32:
+   case aco_opcode::v_sub_i32:
+   case aco_opcode::v_sub_co_u32:
+   case aco_opcode::v_sub_co_u32_e64:
+   case aco_opcode::s_sub_u32:
+   case aco_opcode::s_sub_i32:
+   case aco_opcode::v_subrev_u32:
+   case aco_opcode::v_subrev_co_u32:
+   case aco_opcode::v_subrev_co_u32_e64:
+      ctx.info[instr->definitions[0].tempId()].set_add_sub(instr.get());
+      break;
    case aco_opcode::s_not_b32:
    case aco_opcode::s_not_b64:
       if (!instr->operands[0].isTemp()) {
@@ -1855,8 +1999,9 @@
       } else if (ctx.info[instr->operands[0].tempId()].is_uniform_bitwise()) {
          ctx.info[instr->definitions[0].tempId()].set_uniform_bitwise();
          ctx.info[instr->definitions[1].tempId()].set_scc_invert(
-            ctx.info[instr->operands[0].tempId()].parent_instr->definitions[1].getTemp());
+            ctx.info[instr->operands[0].tempId()].instr->definitions[1].getTemp());
       }
+      ctx.info[instr->definitions[0].tempId()].set_bitwise(instr.get());
       break;
    case aco_opcode::s_and_b32:
    case aco_opcode::s_and_b64:
@@ -1873,9 +2018,9 @@
             /* Try to get rid of the superfluous s_and_b64, since the uniform bitwise instruction
              * already produces the same SCC */
             ctx.info[instr->definitions[1].tempId()].set_temp(
-               ctx.info[instr->operands[0].tempId()].parent_instr->definitions[1].getTemp());
+               ctx.info[instr->operands[0].tempId()].instr->definitions[1].getTemp());
             ctx.info[instr->definitions[0].tempId()].set_uniform_bool(
-               ctx.info[instr->operands[0].tempId()].parent_instr->definitions[1].getTemp());
+               ctx.info[instr->operands[0].tempId()].instr->definitions[1].getTemp());
             break;
          } else if ((ctx.program->stage.num_sw_stages() > 1 ||
                      ctx.program->stage.hw == AC_HW_NEXT_GEN_GEOMETRY_SHADER) &&
@@ -1899,6 +2044,34 @@
                       })) {
          ctx.info[instr->definitions[0].tempId()].set_uniform_bitwise();
       }
+      ctx.info[instr->definitions[0].tempId()].set_bitwise(instr.get());
+      break;
+   case aco_opcode::s_lshl_b32:
+   case aco_opcode::v_or_b32:
+   case aco_opcode::v_lshlrev_b32:
+   case aco_opcode::v_bcnt_u32_b32:
+   case aco_opcode::v_and_b32:
+   case aco_opcode::v_xor_b32:
+   case aco_opcode::v_not_b32:
+      ctx.info[instr->definitions[0].tempId()].set_usedef(instr.get());
+      break;
+   case aco_opcode::v_min_f32:
+   case aco_opcode::v_min_f16:
+   case aco_opcode::v_min_u32:
+   case aco_opcode::v_min_i32:
+   case aco_opcode::v_min_u16:
+   case aco_opcode::v_min_i16:
+   case aco_opcode::v_min_u16_e64:
+   case aco_opcode::v_min_i16_e64:
+   case aco_opcode::v_max_f32:
+   case aco_opcode::v_max_f16:
+   case aco_opcode::v_max_u32:
+   case aco_opcode::v_max_i32:
+   case aco_opcode::v_max_u16:
+   case aco_opcode::v_max_i16:
+   case aco_opcode::v_max_u16_e64:
+   case aco_opcode::v_max_i16_e64:
+      ctx.info[instr->definitions[0].tempId()].set_minmax(instr.get());
       break;
    case aco_opcode::s_cselect_b64:
    case aco_opcode::s_cselect_b32:
@@ -1920,7 +2093,7 @@
       break;
    case aco_opcode::p_extract: {
       if (instr->operands[0].isTemp()) {
-         ctx.info[instr->definitions[0].tempId()].set_extract();
+         ctx.info[instr->definitions[0].tempId()].set_extract(instr.get());
          if (instr->definitions[0].bytes() == 4 && instr->operands[0].regClass() == v1 &&
              parse_insert(instr.get()))
             ctx.info[instr->operands[0].tempId()].set_insert(instr.get());
@@ -1932,13 +2105,29 @@
          if (instr->operands[0].regClass() == v1)
             ctx.info[instr->operands[0].tempId()].set_insert(instr.get());
          if (parse_extract(instr.get()))
-            ctx.info[instr->definitions[0].tempId()].set_extract();
+            ctx.info[instr->definitions[0].tempId()].set_extract(instr.get());
+         ctx.info[instr->definitions[0].tempId()].set_bitwise(instr.get());
       }
       break;
    }
+   case aco_opcode::ds_read_u8:
+   case aco_opcode::ds_read_u8_d16:
+   case aco_opcode::ds_read_u16:
+   case aco_opcode::ds_read_u16_d16: {
+      ctx.info[instr->definitions[0].tempId()].set_usedef(instr.get());
+      break;
+   }
    case aco_opcode::v_cvt_f16_f32: {
+      if (instr->operands[0].isTemp()) {
+         ssa_info& info = ctx.info[instr->operands[0].tempId()];
+         if (!info.is_dpp() || info.instr->pass_flags != instr->pass_flags)
+            info.set_f2f16(instr.get());
+      }
+      break;
+   }
+   case aco_opcode::v_cvt_f32_f16: {
       if (instr->operands[0].isTemp())
-         ctx.info[instr->operands[0].tempId()].set_f2f16(instr.get());
+         ctx.info[instr->definitions[0].tempId()].set_f2f32(instr.get());
       break;
    }
    default: break;
@@ -1948,10 +2137,6 @@
     * neg/abs instructions because we'll likely combine it into another valu. */
    if (!(ctx.info[instr->definitions[0].tempId()].label & (label_neg | label_abs)))
       check_sdwa_extract(ctx, instr);
-
-   /* Set parent_instr for all SSA definitions. */
-   for (const Definition& def : instr->definitions)
-      ctx.info[def.tempId()].parent_instr = instr.get();
 }
 
 unsigned
@@ -1992,19 +2177,15 @@
 Instruction*
 follow_operand(opt_ctx& ctx, Operand op, bool ignore_uses = false)
 {
-   if (!op.isTemp())
+   if (!op.isTemp() || !(ctx.info[op.tempId()].label & instr_usedef_labels))
       return nullptr;
    if (!ignore_uses && ctx.uses[op.tempId()] > 1)
       return nullptr;
 
-   Instruction* instr = ctx.info[op.tempId()].parent_instr;
-
-   if (instr->definitions[0].getTemp() != op.getTemp())
-      return nullptr;
+   Instruction* instr = ctx.info[op.tempId()].instr;
 
    if (instr->definitions.size() == 2) {
-      unsigned idx =
-         instr->definitions[1].isTemp() && instr->definitions[1].tempId() == op.tempId();
+      unsigned idx = ctx.info[op.tempId()].label & label_split ? 1 : 0;
       assert(instr->definitions[idx].isTemp() && instr->definitions[idx].tempId() == op.tempId());
       if (instr->definitions[!idx].isTemp() && ctx.uses[instr->definitions[!idx].tempId()])
          return nullptr;
@@ -2055,8 +2236,6 @@
    cmp->opcode = new_opcode;
    ctx.info[instr->definitions[0].tempId()] = ctx.info[cmp->definitions[0].tempId()];
    std::swap(instr->definitions[0], cmp->definitions[0]);
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-   ctx.info[cmp->definitions[0].tempId()].parent_instr = cmp;
 
    ctx.uses[instr->operands[0].tempId()]--;
    return true;
@@ -2152,7 +2331,6 @@
    new_instr->definitions[0] = instr->definitions[0];
    new_instr->pass_flags = instr->pass_flags;
    ctx.info[instr->definitions[0].tempId()].label = 0;
-   ctx.info[instr->definitions[0].tempId()].parent_instr = new_instr;
 
    instr.reset(new_instr);
 }
@@ -2294,8 +2472,6 @@
    std::swap(instr->definitions[0], op_instr->definitions[0]);
    op_instr->opcode = aco_opcode::v_xnor_b32;
    ctx.info[op_instr->definitions[0].tempId()].label = 0;
-   ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
 
    return true;
 }
@@ -2396,10 +2572,6 @@
    std::swap(instr->definitions[1], op2_instr->definitions[1]);
    ctx.uses[instr->operands[0].tempId()]--;
    ctx.info[op2_instr->definitions[0].tempId()].label = 0;
-   ctx.info[op2_instr->definitions[0].tempId()].parent_instr = op2_instr;
-   ctx.info[op2_instr->definitions[1].tempId()].parent_instr = op2_instr;
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-   ctx.info[instr->definitions[1].tempId()].parent_instr = instr.get();
 
    switch (op2_instr->opcode) {
    case aco_opcode::s_and_b32: op2_instr->opcode = aco_opcode::s_nand_b32; break;
@@ -2497,6 +2669,9 @@
 bool
 combine_sabsdiff(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
+   if (!instr->operands[0].isTemp() || !ctx.info[instr->operands[0].tempId()].is_add_sub())
+      return false;
+
    Instruction* op_instr = follow_operand(ctx, instr->operands[0], false);
    if (!op_instr)
       return false;
@@ -2515,9 +2690,6 @@
          goto use_absdiff;
       }
       return false;
-   } else if (op_instr->opcode != aco_opcode::s_sub_i32 &&
-              op_instr->opcode != aco_opcode::s_sub_u32) {
-      return false;
    }
 
 use_absdiff:
@@ -2526,10 +2698,6 @@
    std::swap(instr->definitions[1], op_instr->definitions[1]);
    ctx.uses[instr->operands[0].tempId()]--;
    ctx.info[op_instr->definitions[0].tempId()].label = 0;
-   ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
-   ctx.info[op_instr->definitions[1].tempId()].parent_instr = op_instr;
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-   ctx.info[instr->definitions[1].tempId()].parent_instr = instr.get();
 
    return true;
 }
@@ -2574,8 +2742,7 @@
          new_instr->operands[2] = Operand(ctx.info[instr->operands[i].tempId()].temp);
          new_instr->pass_flags = instr->pass_flags;
          instr = std::move(new_instr);
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-         ctx.info[instr->definitions[1].tempId()].parent_instr = instr.get();
+         ctx.info[instr->definitions[0].tempId()].set_add_sub(instr.get());
          return true;
       }
    }
@@ -2604,7 +2771,6 @@
          new_instr->pass_flags = instr->pass_flags;
          instr = std::move(new_instr);
          ctx.info[instr->definitions[0].tempId()].label = 0;
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
 
          return true;
       }
@@ -2816,7 +2982,7 @@
       ssa_info& info = ctx.info[instr->operands[i].tempId()];
       if (is_copy_label(ctx, instr, info, i) && info.temp.type() == RegType::sgpr)
          operand_mask |= 1u << i;
-      if (info.is_extract() && info.parent_instr->operands[0].getTemp().type() == RegType::sgpr)
+      if (info.is_extract() && info.instr->operands[0].getTemp().type() == RegType::sgpr)
          operand_mask |= 1u << i;
    }
    unsigned max_sgprs = 1;
@@ -2845,7 +3011,7 @@
 
       ssa_info& info = ctx.info[sgpr_info_id];
 
-      Temp sgpr = info.is_extract() ? info.parent_instr->operands[0].getTemp() : info.temp;
+      Temp sgpr = info.is_extract() ? info.instr->operands[0].getTemp() : info.temp;
       bool new_sgpr = sgpr.id() != sgpr_ids[0] && sgpr.id() != sgpr_ids[1];
       if (new_sgpr && num_sgprs >= max_sgprs)
          continue;
@@ -2948,10 +3114,10 @@
       return false;
    /* if the omod/clamp instruction is dead, then the single user of this
     * instruction is a different instruction */
-   if (!ctx.uses[def_info.mod_instr->definitions[0].tempId()])
+   if (!ctx.uses[def_info.instr->definitions[0].tempId()])
       return false;
 
-   if (def_info.mod_instr->definitions[0].bytes() != instr->definitions[0].bytes())
+   if (def_info.instr->definitions[0].bytes() != instr->definitions[0].bytes())
       return false;
 
    /* MADs/FMAs are created later, so we don't have to update the original add */
@@ -2975,11 +3141,9 @@
    else if (def_info.is_clamp())
       instr->valu().clamp = true;
 
-   instr->definitions[0].swapTemp(def_info.mod_instr->definitions[0]);
+   instr->definitions[0].swapTemp(def_info.instr->definitions[0]);
    ctx.info[instr->definitions[0].tempId()].label &= label_clamp | label_insert | label_f2f16;
-   ctx.uses[def_info.mod_instr->definitions[0].tempId()]--;
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-   ctx.info[def_info.mod_instr->definitions[0].tempId()].parent_instr = def_info.mod_instr;
+   ctx.uses[def_info.instr->definitions[0].tempId()]--;
 
    return true;
 }
@@ -2998,13 +3162,13 @@
       return false;
    /* if the insert instruction is dead, then the single user of this
     * instruction is a different instruction */
-   if (!ctx.uses[def_info.mod_instr->definitions[0].tempId()])
+   if (!ctx.uses[def_info.instr->definitions[0].tempId()])
       return false;
 
    /* MADs/FMAs are created later, so we don't have to update the original add */
    assert(!ctx.info[instr->definitions[0].tempId()].is_mad());
 
-   SubdwordSel sel = parse_insert(def_info.mod_instr);
+   SubdwordSel sel = parse_insert(def_info.instr);
    assert(sel);
 
    if (!can_use_SDWA(ctx.program->gfx_level, instr, true))
@@ -3015,13 +3179,9 @@
       return false;
    instr->sdwa().dst_sel = sel;
 
-   instr->definitions[0].swapTemp(def_info.mod_instr->definitions[0]);
+   instr->definitions[0].swapTemp(def_info.instr->definitions[0]);
    ctx.info[instr->definitions[0].tempId()].label = 0;
-   ctx.uses[def_info.mod_instr->definitions[0].tempId()]--;
-   ctx.info[instr->definitions[0].tempId()].label = 0;
-   ctx.info[def_info.mod_instr->definitions[0].tempId()].parent_instr = def_info.mod_instr;
-   for (const Definition& def : instr->definitions)
-      ctx.info[def.tempId()].parent_instr = instr.get();
+   ctx.uses[def_info.instr->definitions[0].tempId()]--;
 
    return true;
 }
@@ -3030,130 +3190,50 @@
  * p_extract(ds_read_uN(), 0, N, 0) -> ds_read_uN()
  */
 bool
-apply_load_extract(opt_ctx& ctx, aco_ptr<Instruction>& extract)
+apply_ds_extract(opt_ctx& ctx, aco_ptr<Instruction>& extract)
 {
    /* Check if p_extract has a usedef operand and is the only user. */
-   if (ctx.uses[extract->operands[0].tempId()] > 1)
+   if (!ctx.info[extract->operands[0].tempId()].is_usedef() ||
+       ctx.uses[extract->operands[0].tempId()] > 1)
       return false;
 
-   /* Check if the usedef is the right format. */
-   Instruction* load = ctx.info[extract->operands[0].tempId()].parent_instr;
-   if (!load->isDS() && !load->isSMEM() && !load->isMUBUF() && !load->isFlatLike())
+   /* Check if the usedef is a DS instruction. */
+   Instruction* ds = ctx.info[extract->operands[0].tempId()].instr;
+   if (ds->format != Format::DS)
       return false;
 
    unsigned extract_idx = extract->operands[1].constantValue();
    unsigned bits_extracted = extract->operands[2].constantValue();
-   bool sign_ext = extract->operands[3].constantValue();
+   unsigned sign_ext = extract->operands[3].constantValue();
    unsigned dst_bitsize = extract->definitions[0].bytes() * 8u;
 
-   unsigned bits_loaded = 0;
-   bool can_shrink = false;
-   switch (load->opcode) {
-   case aco_opcode::ds_read_u8:
-   case aco_opcode::ds_read_u8_d16:
-   case aco_opcode::flat_load_ubyte:
-   case aco_opcode::flat_load_ubyte_d16:
-   case aco_opcode::global_load_ubyte:
-   case aco_opcode::global_load_ubyte_d16:
-   case aco_opcode::scratch_load_ubyte:
-   case aco_opcode::scratch_load_ubyte_d16: can_shrink = true; FALLTHROUGH;
-   case aco_opcode::s_load_ubyte:
-   case aco_opcode::s_buffer_load_ubyte:
-   case aco_opcode::buffer_load_ubyte:
-   case aco_opcode::buffer_load_ubyte_d16: bits_loaded = 8; break;
-   case aco_opcode::ds_read_u16:
-   case aco_opcode::ds_read_u16_d16:
-   case aco_opcode::flat_load_ushort:
-   case aco_opcode::flat_load_short_d16:
-   case aco_opcode::global_load_ushort:
-   case aco_opcode::global_load_short_d16:
-   case aco_opcode::scratch_load_ushort:
-   case aco_opcode::scratch_load_short_d16: can_shrink = true; FALLTHROUGH;
-   case aco_opcode::s_load_ushort:
-   case aco_opcode::s_buffer_load_ushort:
-   case aco_opcode::buffer_load_ushort:
-   case aco_opcode::buffer_load_short_d16: bits_loaded = 16; break;
-   default: return false;
-   }
-
    /* TODO: These are doable, but probably don't occur too often. */
-   if (extract_idx || bits_extracted > bits_loaded || dst_bitsize > 32 ||
-       (load->definitions[0].regClass().type() != extract->definitions[0].regClass().type()))
+   if (extract_idx || sign_ext || dst_bitsize != 32)
       return false;
 
-   /* We can't shrink some loads because that would remove zeroing of the offset/address LSBs. */
-   if (!can_shrink && bits_extracted < bits_loaded)
+   unsigned bits_loaded = 0;
+   if (ds->opcode == aco_opcode::ds_read_u8 || ds->opcode == aco_opcode::ds_read_u8_d16)
+      bits_loaded = 8;
+   else if (ds->opcode == aco_opcode::ds_read_u16 || ds->opcode == aco_opcode::ds_read_u16_d16)
+      bits_loaded = 16;
+   else
       return false;
 
-   /* Shrink the load if the extracted bit size is smaller. */
+   /* Shrink the DS load if the extracted bit size is smaller. */
    bits_loaded = MIN2(bits_loaded, bits_extracted);
 
-   /* Change the opcode so it writes the full register. */
-   bool is_s_buffer = load->opcode == aco_opcode::s_buffer_load_ubyte ||
-                      load->opcode == aco_opcode::s_buffer_load_ushort;
-   if (bits_loaded == 8 && load->isDS())
-      load->opcode = sign_ext ? aco_opcode::ds_read_i8 : aco_opcode::ds_read_u8;
-   else if (bits_loaded == 16 && load->isDS())
-      load->opcode = sign_ext ? aco_opcode::ds_read_i16 : aco_opcode::ds_read_u16;
-   else if (bits_loaded == 8 && load->isMUBUF())
-      load->opcode = sign_ext ? aco_opcode::buffer_load_sbyte : aco_opcode::buffer_load_ubyte;
-   else if (bits_loaded == 16 && load->isMUBUF())
-      load->opcode = sign_ext ? aco_opcode::buffer_load_sshort : aco_opcode::buffer_load_ushort;
-   else if (bits_loaded == 8 && load->isFlat())
-      load->opcode = sign_ext ? aco_opcode::flat_load_sbyte : aco_opcode::flat_load_ubyte;
-   else if (bits_loaded == 16 && load->isFlat())
-      load->opcode = sign_ext ? aco_opcode::flat_load_sshort : aco_opcode::flat_load_ushort;
-   else if (bits_loaded == 8 && load->isGlobal())
-      load->opcode = sign_ext ? aco_opcode::global_load_sbyte : aco_opcode::global_load_ubyte;
-   else if (bits_loaded == 16 && load->isGlobal())
-      load->opcode = sign_ext ? aco_opcode::global_load_sshort : aco_opcode::global_load_ushort;
-   else if (bits_loaded == 8 && load->isScratch())
-      load->opcode = sign_ext ? aco_opcode::scratch_load_sbyte : aco_opcode::scratch_load_ubyte;
-   else if (bits_loaded == 16 && load->isScratch())
-      load->opcode = sign_ext ? aco_opcode::scratch_load_sshort : aco_opcode::scratch_load_ushort;
-   else if (bits_loaded == 8 && load->isSMEM() && is_s_buffer)
-      load->opcode = sign_ext ? aco_opcode::s_buffer_load_sbyte : aco_opcode::s_buffer_load_ubyte;
-   else if (bits_loaded == 8 && load->isSMEM() && !is_s_buffer)
-      load->opcode = sign_ext ? aco_opcode::s_load_sbyte : aco_opcode::s_load_ubyte;
-   else if (bits_loaded == 16 && load->isSMEM() && is_s_buffer)
-      load->opcode = sign_ext ? aco_opcode::s_buffer_load_sshort : aco_opcode::s_buffer_load_ushort;
-   else if (bits_loaded == 16 && load->isSMEM() && !is_s_buffer)
-      load->opcode = sign_ext ? aco_opcode::s_load_sshort : aco_opcode::s_load_ushort;
+   /* Change the DS opcode so it writes the full register. */
+   if (bits_loaded == 8)
+      ds->opcode = aco_opcode::ds_read_u8;
+   else if (bits_loaded == 16)
+      ds->opcode = aco_opcode::ds_read_u16;
    else
-      unreachable("Forgot to add opcode above.");
-
-   if (dst_bitsize <= 16 && ctx.program->gfx_level >= GFX9) {
-      switch (load->opcode) {
-      case aco_opcode::ds_read_i8: load->opcode = aco_opcode::ds_read_i8_d16; break;
-      case aco_opcode::ds_read_u8: load->opcode = aco_opcode::ds_read_u8_d16; break;
-      case aco_opcode::ds_read_i16: load->opcode = aco_opcode::ds_read_u16_d16; break;
-      case aco_opcode::ds_read_u16: load->opcode = aco_opcode::ds_read_u16_d16; break;
-      case aco_opcode::buffer_load_sbyte: load->opcode = aco_opcode::buffer_load_sbyte_d16; break;
-      case aco_opcode::buffer_load_ubyte: load->opcode = aco_opcode::buffer_load_ubyte_d16; break;
-      case aco_opcode::buffer_load_sshort: load->opcode = aco_opcode::buffer_load_short_d16; break;
-      case aco_opcode::buffer_load_ushort: load->opcode = aco_opcode::buffer_load_short_d16; break;
-      case aco_opcode::flat_load_sbyte: load->opcode = aco_opcode::flat_load_sbyte_d16; break;
-      case aco_opcode::flat_load_ubyte: load->opcode = aco_opcode::flat_load_ubyte_d16; break;
-      case aco_opcode::flat_load_sshort: load->opcode = aco_opcode::flat_load_short_d16; break;
-      case aco_opcode::flat_load_ushort: load->opcode = aco_opcode::flat_load_short_d16; break;
-      case aco_opcode::global_load_sbyte: load->opcode = aco_opcode::global_load_sbyte_d16; break;
-      case aco_opcode::global_load_ubyte: load->opcode = aco_opcode::global_load_ubyte_d16; break;
-      case aco_opcode::global_load_sshort: load->opcode = aco_opcode::global_load_short_d16; break;
-      case aco_opcode::global_load_ushort: load->opcode = aco_opcode::global_load_short_d16; break;
-      case aco_opcode::scratch_load_sbyte: load->opcode = aco_opcode::scratch_load_sbyte_d16; break;
-      case aco_opcode::scratch_load_ubyte: load->opcode = aco_opcode::scratch_load_ubyte_d16; break;
-      case aco_opcode::scratch_load_sshort: load->opcode = aco_opcode::scratch_load_short_d16; break;
-      case aco_opcode::scratch_load_ushort: load->opcode = aco_opcode::scratch_load_short_d16; break;
-      default: break;
-      }
-   }
+      unreachable("Forgot to add DS opcode above.");
 
-   /* The load now produces the exact same thing as the extract, remove the extract. */
-   std::swap(load->definitions[0], extract->definitions[0]);
+   /* The DS now produces the exact same thing as the extract, remove the extract. */
+   std::swap(ds->definitions[0], extract->definitions[0]);
    ctx.uses[extract->definitions[0].tempId()] = 0;
-   ctx.info[load->definitions[0].tempId()].label = 0;
-   ctx.info[extract->definitions[0].tempId()].parent_instr = extract.get();
-   ctx.info[load->definitions[0].tempId()].parent_instr = load;
+   ctx.info[ds->definitions[0].tempId()].label = 0;
    return true;
 }
 
@@ -3195,7 +3275,6 @@
          instr.reset(new_instr);
          decrease_uses(ctx, op_instr);
          ctx.info[instr->definitions[0].tempId()].label = 0;
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
          return true;
       }
    }
@@ -3262,7 +3341,6 @@
          new_instr->pass_flags = instr->pass_flags;
          instr = std::move(new_instr);
          ctx.info[instr->definitions[0].tempId()].label = 0;
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
          return true;
       }
    }
@@ -3299,13 +3377,13 @@
        vop3p->clamp && instr->operands[0].isTemp() && ctx.uses[instr->operands[0].tempId()] == 1 &&
        !vop3p->opsel_lo[1] && !vop3p->opsel_hi[1]) {
 
-      Instruction* op_instr = ctx.info[instr->operands[0].tempId()].parent_instr;
-      if (op_instr->isVOP3P() && instr_info.can_use_output_modifiers[(int)op_instr->opcode]) {
-         op_instr->valu().clamp = true;
-         propagate_swizzles(&op_instr->valu(), vop3p->opsel_lo[0], vop3p->opsel_hi[0]);
-         instr->definitions[0].swapTemp(op_instr->definitions[0]);
-         ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+      ssa_info& info = ctx.info[instr->operands[0].tempId()];
+      if (info.is_vop3p() && instr_info.can_use_output_modifiers[(int)info.instr->opcode]) {
+         VALU_instruction* candidate = &ctx.info[instr->operands[0].tempId()].instr->valu();
+         candidate->clamp = true;
+         propagate_swizzles(candidate, vop3p->opsel_lo[0], vop3p->opsel_hi[0]);
+         instr->definitions[0].swapTemp(candidate->definitions[0]);
+         ctx.info[candidate->definitions[0].tempId()].instr = candidate;
          ctx.uses[instr->definitions[0].tempId()]--;
          return;
       }
@@ -3320,11 +3398,11 @@
          continue;
 
       ssa_info& info = ctx.info[op.tempId()];
-      if (info.parent_instr->opcode == aco_opcode::v_pk_mul_f16 &&
-          (info.parent_instr->operands[0].constantEquals(0x3C00) ||
-           info.parent_instr->operands[1].constantEquals(0x3C00))) {
+      if (info.is_vop3p() && info.instr->opcode == aco_opcode::v_pk_mul_f16 &&
+          (info.instr->operands[0].constantEquals(0x3C00) ||
+           info.instr->operands[1].constantEquals(0x3C00))) {
 
-         VALU_instruction* fneg = &info.parent_instr->valu();
+         VALU_instruction* fneg = &info.instr->valu();
 
          unsigned fneg_src = fneg->operands[0].constantEquals(0x3C00);
 
@@ -3378,7 +3456,7 @@
          if (!op_instr)
             continue;
 
-         if (op_instr->isVOP3P()) {
+         if (ctx.info[instr->operands[i].tempId()].is_vop3p()) {
             if (fadd) {
                if (op_instr->opcode != aco_opcode::v_pk_mul_f16 ||
                    op_instr->definitions[0].isPrecise())
@@ -3461,7 +3539,7 @@
       fma->definitions[0] = instr->definitions[0];
       fma->pass_flags = instr->pass_flags;
       instr = std::move(fma);
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+      ctx.info[instr->definitions[0].tempId()].set_vop3p(instr.get());
       decrease_uses(ctx, mul_instr);
       return;
    }
@@ -3496,7 +3574,7 @@
 void
 to_mad_mix(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
-   ctx.info[instr->definitions[0].tempId()].label &= label_f2f16 | label_clamp;
+   ctx.info[instr->definitions[0].tempId()].label &= label_f2f16 | label_clamp | label_mul;
 
    if (instr->opcode == aco_opcode::v_fma_f32) {
       instr->format = (Format)((uint32_t)withoutVOP3(instr->format) | (uint32_t)(Format::VOP3P));
@@ -3527,7 +3605,9 @@
    vop3p->valu().clamp = instr->valu().clamp;
    vop3p->pass_flags = instr->pass_flags;
    instr = std::move(vop3p);
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+
+   if (ctx.info[instr->definitions[0].tempId()].label & label_mul)
+      ctx.info[instr->definitions[0].tempId()].instr = instr.get();
 }
 
 bool
@@ -3536,7 +3616,7 @@
    ssa_info& def_info = ctx.info[instr->definitions[0].tempId()];
    if (!def_info.is_f2f16())
       return false;
-   Instruction* conv = def_info.mod_instr;
+   Instruction* conv = def_info.instr;
 
    if (!ctx.uses[conv->definitions[0].tempId()] || ctx.uses[instr->definitions[0].tempId()] != 1)
       return false;
@@ -3559,8 +3639,6 @@
       instr->definitions[0].setPrecise(true);
    ctx.info[instr->definitions[0].tempId()].label &= label_clamp;
    ctx.uses[conv->definitions[0].tempId()]--;
-   ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
-   ctx.info[conv->definitions[0].tempId()].parent_instr = conv;
 
    return true;
 }
@@ -3575,10 +3653,11 @@
       if (!instr->operands[i].isTemp())
          continue;
       Temp tmp = instr->operands[i].getTemp();
+      if (!ctx.info[tmp.id()].is_f2f32())
+         continue;
 
-      Instruction* conv = ctx.info[tmp.id()].parent_instr;
-      if (conv->opcode != aco_opcode::v_cvt_f32_f16 || !conv->operands[0].isTemp() ||
-          conv->valu().clamp || conv->valu().omod) {
+      Instruction* conv = ctx.info[tmp.id()].instr;
+      if (conv->valu().clamp || conv->valu().omod) {
          continue;
       } else if (conv->isSDWA() &&
                  (conv->sdwa().dst_sel.size() != 4 || conv->sdwa().sel[0].size() != 2)) {
@@ -3657,21 +3736,6 @@
    }
 }
 
-bool
-is_mul(Instruction* instr)
-{
-   switch (instr->opcode) {
-   case aco_opcode::v_mul_f64_e64:
-   case aco_opcode::v_mul_f64:
-   case aco_opcode::v_mul_f32:
-   case aco_opcode::v_mul_legacy_f32:
-   case aco_opcode::v_mul_f16: return true;
-   case aco_opcode::v_fma_mix_f32:
-      return instr->operands[2].constantEquals(0) && instr->valu().neg[2];
-   default: return false;
-   }
-}
-
 void
 combine_instruction(opt_ctx& ctx, aco_ptr<Instruction>& instr)
 {
@@ -3695,14 +3759,14 @@
             continue;
          }
          if (info.is_extract() &&
-             (info.parent_instr->operands[0].getTemp().type() == RegType::vgpr ||
+             (info.instr->operands[0].getTemp().type() == RegType::vgpr ||
               instr->operands[i].getTemp().type() == RegType::sgpr) &&
              can_apply_extract(ctx, instr, i, info)) {
             /* Increase use count of the extract's operand if the extract still has uses. */
             apply_extract(ctx, instr, i, info);
             if (--ctx.uses[instr->operands[i].tempId()])
-               ctx.uses[info.parent_instr->operands[0].tempId()]++;
-            instr->operands[i].setTemp(info.parent_instr->operands[0].getTemp());
+               ctx.uses[info.instr->operands[0].tempId()]++;
+            instr->operands[i].setTemp(info.instr->operands[0].getTemp());
          }
       }
    }
@@ -3728,12 +3792,12 @@
       if (info.is_extract() && can_apply_extract(ctx, instr, 0, info)) {
          apply_extract(ctx, instr, 0, info);
          if (--ctx.uses[instr->operands[0].tempId()])
-            ctx.uses[info.parent_instr->operands[0].tempId()]++;
-         instr->operands[0].setTemp(info.parent_instr->operands[0].getTemp());
+            ctx.uses[info.instr->operands[0].tempId()]++;
+         instr->operands[0].setTemp(info.instr->operands[0].getTemp());
       }
 
       if (instr->opcode == aco_opcode::p_extract)
-         apply_load_extract(ctx, instr);
+         apply_ds_extract(ctx, instr);
    }
 
    /* TODO: There are still some peephole optimizations that could be done:
@@ -3750,11 +3814,12 @@
    if ((ctx.info[instr->definitions[0].tempId()].label & (label_neg | label_abs)) &&
        ctx.uses[instr->operands[1].tempId()] == 1) {
       Temp val = ctx.info[instr->definitions[0].tempId()].temp;
-      Instruction* mul_instr = ctx.info[val.id()].parent_instr;
 
-      if (!is_mul(mul_instr))
+      if (!ctx.info[val.id()].is_mul())
          return;
 
+      Instruction* mul_instr = ctx.info[val.id()].instr;
+
       if (mul_instr->operands[0].isLiteral())
          return;
       if (mul_instr->valu().clamp)
@@ -3793,7 +3858,7 @@
       new_mul.neg[0] ^= is_neg;
       new_mul.clamp = false;
 
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
+      ctx.info[instr->definitions[0].tempId()].set_mul(instr.get());
       return;
    }
 
@@ -3818,36 +3883,33 @@
       bool emit_fma = false;
       /* find the 'best' mul instruction to combine with the add */
       for (unsigned i = is_add_mix ? 1 : 0; i < instr->operands.size(); i++) {
-         if (!instr->operands[i].isTemp())
+         if (!instr->operands[i].isTemp() || !ctx.info[instr->operands[i].tempId()].is_mul())
             continue;
          ssa_info& info = ctx.info[instr->operands[i].tempId()];
-         if (!is_mul(info.parent_instr))
-            continue;
 
          /* no clamp/omod allowed between mul and add */
-         if (info.parent_instr->isVOP3() &&
-             (info.parent_instr->valu().clamp || info.parent_instr->valu().omod))
+         if (info.instr->isVOP3() && (info.instr->valu().clamp || info.instr->valu().omod))
             continue;
-         if (info.parent_instr->isVOP3P() && info.parent_instr->valu().clamp)
+         if (info.instr->isVOP3P() && info.instr->valu().clamp)
             continue;
          /* v_fma_mix_f32/etc can't do omod */
-         if (info.parent_instr->isVOP3P() && instr->isVOP3() && instr->valu().omod)
+         if (info.instr->isVOP3P() && instr->isVOP3() && instr->valu().omod)
             continue;
          /* don't promote fp16 to fp32 or remove fp32->fp16->fp32 conversions */
-         if (is_add_mix && info.parent_instr->definitions[0].bytes() == 2)
+         if (is_add_mix && info.instr->definitions[0].bytes() == 2)
             continue;
 
-         if (get_operand_size(instr, i) != info.parent_instr->definitions[0].bytes() * 8)
+         if (get_operand_size(instr, i) != info.instr->definitions[0].bytes() * 8)
             continue;
 
-         bool legacy = info.parent_instr->opcode == aco_opcode::v_mul_legacy_f32;
-         bool mad_mix = is_add_mix || info.parent_instr->isVOP3P();
+         bool legacy = info.instr->opcode == aco_opcode::v_mul_legacy_f32;
+         bool mad_mix = is_add_mix || info.instr->isVOP3P();
 
          /* Multiplication by power-of-two should never need rounding. 1/power-of-two also works,
           * but using fma removes denormal flushing (0xfffffe * 0.5 + 0x810001a2).
           */
-         bool is_fma_precise = is_pow_of_two(ctx, info.parent_instr->operands[0]) ||
-                               is_pow_of_two(ctx, info.parent_instr->operands[1]);
+         bool is_fma_precise = is_pow_of_two(ctx, info.instr->operands[0]) ||
+                               is_pow_of_two(ctx, info.instr->operands[1]);
 
          bool has_fma = mad16 || mad64 || (legacy && ctx.program->gfx_level >= GFX10_3) ||
                         (mad32 && !legacy && !mad_mix && ctx.program->dev.has_fast_fma32) ||
@@ -3855,9 +3917,10 @@
          bool has_mad = mad_mix ? !ctx.program->dev.fused_mad_mix
                                 : ((mad32 && ctx.program->gfx_level < GFX10_3) ||
                                    (mad16 && ctx.program->gfx_level <= GFX9));
-         bool can_use_fma = has_fma && (!(info.parent_instr->definitions[0].isPrecise() ||
-                                          instr->definitions[0].isPrecise()) ||
-                                        is_fma_precise);
+         bool can_use_fma =
+            has_fma &&
+            (!(info.instr->definitions[0].isPrecise() || instr->definitions[0].isPrecise()) ||
+             is_fma_precise);
          bool can_use_mad =
             has_mad && (mad_mix || mad32 ? ctx.fp_mode.denorm32 : ctx.fp_mode.denorm16_64) == 0;
          if (mad_mix && legacy)
@@ -3866,20 +3929,20 @@
             continue;
 
          unsigned candidate_add_op_idx = is_add_mix ? (3 - i) : (1 - i);
-         Operand op[3] = {info.parent_instr->operands[0], info.parent_instr->operands[1],
+         Operand op[3] = {info.instr->operands[0], info.instr->operands[1],
                           instr->operands[candidate_add_op_idx]};
-         if (info.parent_instr->isSDWA() || info.parent_instr->isDPP() ||
-             !check_vop3_operands(ctx, 3, op) || ctx.uses[instr->operands[i].tempId()] > uses)
+         if (info.instr->isSDWA() || info.instr->isDPP() || !check_vop3_operands(ctx, 3, op) ||
+             ctx.uses[instr->operands[i].tempId()] > uses)
             continue;
 
          if (ctx.uses[instr->operands[i].tempId()] == uses) {
             unsigned cur_idx = mul_instr->definitions[0].tempId();
-            unsigned new_idx = info.parent_instr->definitions[0].tempId();
+            unsigned new_idx = info.instr->definitions[0].tempId();
             if (cur_idx > new_idx)
                continue;
          }
 
-         mul_instr = info.parent_instr;
+         mul_instr = info.instr;
          add_op_idx = candidate_add_op_idx;
          uses = ctx.uses[instr->operands[i].tempId()];
          emit_fma = !can_use_mad;
@@ -3989,7 +4052,6 @@
          /* mark this ssa_def to be re-checked for profitability and literals */
          ctx.mad_infos.emplace_back(std::move(add_instr), mul_instr->definitions[0].tempId());
          ctx.info[instr->definitions[0].tempId()].set_mad(ctx.mad_infos.size() - 1);
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
          return;
       }
    }
@@ -4015,7 +4077,6 @@
             new_instr->pass_flags = instr->pass_flags;
             instr = std::move(new_instr);
             ctx.info[instr->definitions[0].tempId()].label = 0;
-            ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
             return;
          }
       }
@@ -4177,7 +4238,6 @@
          it->second.block = block_idx;
          ctx.uses.push_back(0);
          ctx.info.push_back(ctx.info[op.tempId()]);
-         ctx.info[it->second.instr->definitions[0].tempId()].parent_instr = it->second.instr;
       }
 
       /* Use the rematerialized constant and update information about latest use. */
@@ -4263,7 +4323,7 @@
           * divergent users), and it also makes sure that the current instruction will keep working
           * even if the predecessor won't be transformed.
           */
-         Instruction* pred_instr = ctx.info[op.tempId()].parent_instr;
+         Instruction* pred_instr = ctx.info[op.tempId()].instr;
          assert(pred_instr->definitions.size() >= 2);
          assert(pred_instr->definitions[1].isFixed() &&
                 pred_instr->definitions[1].physReg() == scc);
@@ -4306,9 +4366,9 @@
          }
       }
       bool done = false;
-      Instruction* vec = ctx.info[instr->operands[0].tempId()].parent_instr;
-      if (num_used == 1 && vec->opcode == aco_opcode::p_create_vector &&
+      if (num_used == 1 && ctx.info[instr->operands[0].tempId()].is_vec() &&
           ctx.uses[instr->operands[0].tempId()] == 1) {
+         Instruction* vec = ctx.info[instr->operands[0].tempId()].instr;
 
          unsigned off = 0;
          Operand op;
@@ -4333,7 +4393,6 @@
             copy->operands[0] = op;
             copy->definitions[0] = instr->definitions[idx];
             instr = std::move(copy);
-            ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
 
             done = true;
          }
@@ -4349,7 +4408,6 @@
             Operand::c32((uint32_t)split_offset / instr->definitions[idx].bytes());
          extract->definitions[0] = instr->definitions[idx];
          instr = std::move(extract);
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
       }
    }
 
@@ -4364,7 +4422,6 @@
          if (instr->operands[1].isTemp())
             ctx.uses[instr->operands[1].tempId()]--;
          instr.swap(mad_info->add_instr);
-         ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
          mad_info = NULL;
       }
       /* check literals */
@@ -4506,9 +4563,8 @@
           ctx.uses[instr->definitions[1].tempId()] == 0 &&
           can_eliminate_and_exec(ctx, instr->operands[0].getTemp(), instr->pass_flags)) {
          ctx.uses[instr->operands[0].tempId()]--;
-         Instruction* op_instr = ctx.info[instr->operands[0].tempId()].parent_instr;
-         op_instr->definitions[0].setTemp(instr->definitions[0].getTemp());
-         ctx.info[op_instr->definitions[0].tempId()].parent_instr = op_instr;
+         ctx.info[instr->operands[0].tempId()].instr->definitions[0].setTemp(
+            instr->definitions[0].getTemp());
          instr.reset();
          return;
       }
@@ -4521,8 +4577,7 @@
             continue;
          ssa_info info = ctx.info[instr->operands[i].tempId()];
 
-         if (!info.parent_instr->isDPP() || info.parent_instr->opcode != aco_opcode::v_mov_b32 ||
-             info.parent_instr->pass_flags != instr->pass_flags)
+         if (!info.is_dpp() || info.instr->pass_flags != instr->pass_flags)
             continue;
 
          /* We won't eliminate the DPP mov if the operand is used twice */
@@ -4538,13 +4593,13 @@
             instr->valu().swapOperands(0, i);
          }
 
-         bool dpp8 = info.parent_instr->isDPP8();
-         if (!can_use_DPP(ctx.program->gfx_level, instr, dpp8))
+         if (!can_use_DPP(ctx.program->gfx_level, instr, info.is_dpp8()))
             continue;
 
+         bool dpp8 = info.is_dpp8();
          bool input_mods = can_use_input_modifiers(ctx.program->gfx_level, instr->opcode, 0) &&
                            get_operand_size(instr, 0) == 32;
-         bool mov_uses_mods = info.parent_instr->valu().neg[0] || info.parent_instr->valu().abs[0];
+         bool mov_uses_mods = info.instr->valu().neg[0] || info.instr->valu().abs[0];
          if (((dpp8 && ctx.program->gfx_level < GFX11) || !input_mods) && mov_uses_mods)
             continue;
 
@@ -4552,28 +4607,23 @@
 
          if (dpp8) {
             DPP8_instruction* dpp = &instr->dpp8();
-            dpp->lane_sel = info.parent_instr->dpp8().lane_sel;
-            dpp->fetch_inactive = info.parent_instr->dpp8().fetch_inactive;
+            dpp->lane_sel = info.instr->dpp8().lane_sel;
+            dpp->fetch_inactive = info.instr->dpp8().fetch_inactive;
             if (mov_uses_mods)
                instr->format = asVOP3(instr->format);
          } else {
             DPP16_instruction* dpp = &instr->dpp16();
-            /* anything else doesn't make sense in SSA */
-            assert(info.parent_instr->dpp16().row_mask == 0xf &&
-                   info.parent_instr->dpp16().bank_mask == 0xf);
-            dpp->dpp_ctrl = info.parent_instr->dpp16().dpp_ctrl;
-            dpp->bound_ctrl = info.parent_instr->dpp16().bound_ctrl;
-            dpp->fetch_inactive = info.parent_instr->dpp16().fetch_inactive;
-         }
-
-         instr->valu().neg[0] ^= info.parent_instr->valu().neg[0] && !instr->valu().abs[0];
-         instr->valu().abs[0] |= info.parent_instr->valu().abs[0];
-
-         if (--ctx.uses[info.parent_instr->definitions[0].tempId()])
-            ctx.uses[info.parent_instr->operands[0].tempId()]++;
-         instr->operands[0].setTemp(info.parent_instr->operands[0].getTemp());
-         for (const Definition& def : instr->definitions)
-            ctx.info[def.tempId()].parent_instr = instr.get();
+            dpp->dpp_ctrl = info.instr->dpp16().dpp_ctrl;
+            dpp->bound_ctrl = info.instr->dpp16().bound_ctrl;
+            dpp->fetch_inactive = info.instr->dpp16().fetch_inactive;
+         }
+
+         instr->valu().neg[0] ^= info.instr->valu().neg[0] && !instr->valu().abs[0];
+         instr->valu().abs[0] |= info.instr->valu().abs[0];
+
+         if (--ctx.uses[info.instr->definitions[0].tempId()])
+            ctx.uses[info.instr->operands[0].tempId()]++;
+         instr->operands[0].setTemp(info.instr->operands[0].getTemp());
          break;
       }
    }
@@ -4599,7 +4649,6 @@
       fma->valu().neg[2] = true;
       instr.reset(fma);
       ctx.info[instr->definitions[0].tempId()].label = 0;
-      ctx.info[instr->definitions[0].tempId()].parent_instr = instr.get();
    }
 
    if (instr->isSDWA() || (instr->isVOP3() && ctx.program->gfx_level < GFX10) ||
@@ -4933,51 +4982,6 @@
    ctx.instructions.emplace_back(std::move(instr));
 }
 
-void
-validate_opt_ctx(opt_ctx& ctx)
-{
-   if (!(debug_flags & DEBUG_VALIDATE_OPT))
-      return;
-
-   Program* program = ctx.program;
-
-   bool is_valid = true;
-   auto check = [&program, &is_valid](bool success, const char* msg,
-                                      aco::Instruction* instr) -> void
-   {
-      if (!success) {
-         char* out;
-         size_t outsize;
-         struct u_memstream mem;
-         u_memstream_open(&mem, &out, &outsize);
-         FILE* const memf = u_memstream_get(&mem);
-
-         fprintf(memf, "Optimizer: %s: ", msg);
-         aco_print_instr(program->gfx_level, instr, memf);
-         u_memstream_close(&mem);
-
-         aco_err(program, "%s", out);
-         free(out);
-
-         is_valid = false;
-      }
-   };
-
-   for (Block& block : program->blocks) {
-      for (aco_ptr<Instruction>& instr : block.instructions) {
-         if (!instr)
-            continue;
-         for (const Definition& def : instr->definitions) {
-            check(ctx.info[def.tempId()].parent_instr == instr.get(), "parent_instr incorrect",
-                  instr.get());
-         }
-      }
-   }
-   if (!is_valid) {
-      abort();
-   }
-}
-
 } /* end namespace */
 
 void
@@ -4994,15 +4998,11 @@
          label_instruction(ctx, instr);
    }
 
-   validate_opt_ctx(ctx);
-
    ctx.uses = dead_code_analysis(program);
 
    /* 2. Rematerialize constants in every block. */
    rematerialize_constants(ctx);
 
-   validate_opt_ctx(ctx);
-
    /* 3. Combine v_mad, omod, clamp and propagate sgpr on VALU instructions */
    for (Block& block : program->blocks) {
       ctx.fp_mode = block.fp_mode;
@@ -5010,8 +5010,6 @@
          combine_instruction(ctx, instr);
    }
 
-   validate_opt_ctx(ctx);
-
    /* 4. Top-Down DAG pass (backward) to select instructions (includes DCE) */
    for (auto block_rit = program->blocks.rbegin(); block_rit != program->blocks.rend();
         ++block_rit) {
@@ -5022,8 +5020,6 @@
          select_instruction(ctx, *instr_rit);
    }
 
-   validate_opt_ctx(ctx);
-
    /* 5. Add literals to instructions */
    for (Block& block : program->blocks) {
       ctx.instructions.reserve(block.instructions.size());
@@ -5032,8 +5028,6 @@
          apply_literals(ctx, instr);
       block.instructions = std::move(ctx.instructions);
    }
-
-   validate_opt_ctx(ctx);
 }
 
 } // namespace aco
--- mesa-chaotic/src/compiler/builtin_types.py	2025-05-11 17:14:25.557667099 +0200
+++ mesa-dadschoorse/src/compiler/builtin_types.py	2025-05-11 17:09:56.144631856 +0200
@@ -62,6 +62,7 @@
 vector_type("uint8_t",   "u8vec",  "GLSL_TYPE_UINT8",   "GL_UNSIGNED_INT8", "_NV")
 
 vector_type("bfloat16_t", "bf16vec", "GLSL_TYPE_BFLOAT16", None)
+vector_type("e4m3fn_t", "e4m3fnvec", "GLSL_TYPE_FLOAT_E4M3FN", None)
 
 simple_type("mat2",   "GL_FLOAT_MAT2",   "GLSL_TYPE_FLOAT", 2, 2)
 simple_type("mat3",   "GL_FLOAT_MAT3",   "GLSL_TYPE_FLOAT", 3, 3)
--- mesa-chaotic/docs/features.txt	2025-05-11 17:14:25.466064911 +0200
+++ mesa-dadschoorse/docs/features.txt	2025-05-11 17:09:56.040966796 +0200
@@ -559,7 +559,7 @@
   VK_KHR_ray_tracing_maintenance1                       DONE (anv/gfx12.5+, radv/gfx10.3+, tu/a740+, vn)
   VK_KHR_ray_tracing_pipeline                           DONE (anv/gfx12.5+, lvp, radv/gfx10.3+, vn)
   VK_KHR_ray_tracing_position_fetch                     DONE (anv, radv/gfx10.3+, vn)
-  VK_KHR_shader_bfloat16                                DONE (anv/gfx12.5+, radv/gfx12+)
+  VK_KHR_shader_bfloat16                                DONE (anv/gfx12.5+, radv/gfx11+)
   VK_KHR_shader_clock                                   DONE (anv, hasvk, lvp, nvk, radv, tu, vn)
   VK_KHR_shader_maximal_reconvergence                   DONE (anv, lvp, nvk, panvk/v10+, radv, vn)
   VK_KHR_shader_relaxed_extended_instruction            DONE (anv, hasvk, nvk, panvk, radv, tu, v3dv, vn)
--- mesa-chaotic/src/compiler/glsl_types.c	2025-05-11 17:14:25.570826936 +0200
+++ mesa-dadschoorse/src/compiler/glsl_types.c	2025-05-11 17:09:56.158476925 +0200
@@ -349,6 +349,8 @@
       return &glsl_type_builtin_double;
    case GLSL_TYPE_BFLOAT16:
       return &glsl_type_builtin_bfloat16_t;
+   case GLSL_TYPE_FLOAT_E4M3FN:
+      return &glsl_type_builtin_e4m3fn_t;
    case GLSL_TYPE_BOOL:
       return &glsl_type_builtin_bool;
    case GLSL_TYPE_UINT64:
@@ -387,6 +389,7 @@
    case GLSL_TYPE_INT16:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_UINT:
    case GLSL_TYPE_INT:
    case GLSL_TYPE_FLOAT:
@@ -597,6 +600,7 @@
 VECN(components, float, vec)
 VECN(components, float16_t, f16vec)
 VECN(components, bfloat16_t, bf16vec)
+VECN(components, e4m3fn_t, e4m3fnvec)
 VECN(components, double, dvec)
 VECN(components, int, ivec)
 VECN(components, uint, uvec)
@@ -647,6 +651,8 @@
          return glsl_f16vec_type(rows);
       case GLSL_TYPE_BFLOAT16:
          return glsl_bf16vec_type(rows);
+      case GLSL_TYPE_FLOAT_E4M3FN:
+         return glsl_e4m3fnvec_type(rows);
       case GLSL_TYPE_DOUBLE:
          return glsl_dvec_type(rows);
       case GLSL_TYPE_BOOL:
@@ -1749,6 +1755,7 @@
    case GLSL_TYPE_FLOAT:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_BOOL:
       return glsl_get_components(t);
 
@@ -1802,6 +1809,7 @@
    case GLSL_TYPE_FLOAT:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_BOOL:
       return glsl_get_components(t);
 
@@ -2889,6 +2897,7 @@
    case GLSL_TYPE_FLOAT:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_BOOL:
       return t->matrix_columns;
    case GLSL_TYPE_DOUBLE:
@@ -3094,6 +3103,7 @@
    case GLSL_TYPE_FLOAT:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_DOUBLE:
    case GLSL_TYPE_UINT8:
    case GLSL_TYPE_INT8:
@@ -3743,6 +3753,7 @@
    case GLSL_TYPE_INT16:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_UINT:
    case GLSL_TYPE_INT:
    case GLSL_TYPE_FLOAT:
@@ -3803,6 +3814,7 @@
    case GLSL_TYPE_INT16:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_UINT:
    case GLSL_TYPE_INT:
    case GLSL_TYPE_FLOAT:
@@ -3863,6 +3875,7 @@
    case GLSL_TYPE_INT16:
    case GLSL_TYPE_FLOAT16:
    case GLSL_TYPE_BFLOAT16:
+   case GLSL_TYPE_FLOAT_E4M3FN:
    case GLSL_TYPE_UINT:
    case GLSL_TYPE_INT:
    case GLSL_TYPE_FLOAT:
--- mesa-chaotic/src/compiler/glsl_types.h	2025-05-11 17:14:25.570826936 +0200
+++ mesa-dadschoorse/src/compiler/glsl_types.h	2025-05-11 17:09:56.158476925 +0200
@@ -64,6 +64,7 @@
    GLSL_TYPE_FLOAT,
    GLSL_TYPE_FLOAT16,
    GLSL_TYPE_BFLOAT16,
+   GLSL_TYPE_FLOAT_E4M3FN,
    GLSL_TYPE_DOUBLE,
    GLSL_TYPE_UINT8,
    GLSL_TYPE_INT8,
@@ -107,6 +108,7 @@
 
    case GLSL_TYPE_UINT8:
    case GLSL_TYPE_INT8:
+   case GLSL_TYPE_FLOAT_E4M3FN:
       return 8;
 
    case GLSL_TYPE_DOUBLE:
@@ -176,6 +178,7 @@
 
    case GLSL_TYPE_UINT8:
    case GLSL_TYPE_INT8:
+   case GLSL_TYPE_FLOAT_E4M3FN:
       return 8;
 
    case GLSL_TYPE_DOUBLE:
@@ -631,6 +634,12 @@
 }
 
 static inline bool
+glsl_type_is_e4m3fn(const glsl_type *t)
+{
+   return t->base_type == GLSL_TYPE_FLOAT_E4M3FN;
+}
+
+static inline bool
 glsl_type_is_int_16_32_64(const glsl_type *t)
 {
    return t->base_type == GLSL_TYPE_INT16 ||
@@ -947,6 +956,7 @@
 static inline const glsl_type *glsl_bool_type(void) { return &glsl_type_builtin_bool; }
 static inline const glsl_type *glsl_atomic_uint_type(void) { return &glsl_type_builtin_atomic_uint; }
 static inline const glsl_type *glsl_bfloat16_t_type(void) { return &glsl_type_builtin_bfloat16_t; }
+static inline const glsl_type *glsl_e4m3fn_t_type(void) { return &glsl_type_builtin_e4m3fn_t; }
 
 static inline const glsl_type *
 glsl_floatN_t_type(unsigned bit_size)
@@ -999,6 +1009,7 @@
 const glsl_type *glsl_vec_type(unsigned components);
 const glsl_type *glsl_f16vec_type(unsigned components);
 const glsl_type *glsl_bf16vec_type(unsigned components);
+const glsl_type *glsl_e4m3fnvec_type(unsigned components);
 const glsl_type *glsl_dvec_type(unsigned components);
 const glsl_type *glsl_ivec_type(unsigned components);
 const glsl_type *glsl_uvec_type(unsigned components);
--- mesa-chaotic/docs/relnotes/new_features.txt	2025-05-11 17:14:25.488064586 +0200
+++ mesa-dadschoorse/docs/relnotes/new_features.txt	2025-05-11 17:09:56.060771403 +0200
@@ -19,5 +19,4 @@
 VK_EXT_shader_demote_to_helper_invocation on panvk
 VK_EXT_shader_replicated_composites on panvk
 VK_EXT_depth_bias_control on panvk
-VK_KHR_shader_bfloat16 on anv/gfx125+ and radv/gfx12+
-VK_KHR_robustness2 on RADV
+VK_KHR_shader_bfloat16 on anv/gfx125+ and radv/gfx11+
--- mesa-chaotic/src/amd/vulkan/nir/radv_nir_lower_cooperative_matrix.c	2025-05-11 17:14:25.533608713 +0200
+++ mesa-dadschoorse/src/amd/vulkan/nir/radv_nir_lower_cooperative_matrix.c	2025-05-11 17:09:56.115602879 +0200
@@ -166,13 +166,15 @@
    if (params->gfx_level >= GFX12) {
       base_row = nir_udiv_imm(b, local_idx, 16);
 
-      if (desc.use == GLSL_CMAT_USE_ACCUMULATOR && params->wave_size == 64) {
+      if ((desc.use == GLSL_CMAT_USE_ACCUMULATOR || radv_nir_cmat_bits(desc) == 8) && params->wave_size == 64) {
          /* Switch rows from lanes 16..31 to 32..47, offset right shift by -2
           * to get implicit * 4.
           */
          base_row = nir_ushr_imm(b, nir_bitfield_reverse(b, base_row), 30 - 2);
+      } else if ((desc.use == GLSL_CMAT_USE_ACCUMULATOR || radv_nir_cmat_bits(desc) == 8) && params->wave_size == 32) {
+         base_row = nir_imul_imm(b, base_row, 8);
       } else {
-         base_row = nir_imul_imm(b, base_row, desc.use == GLSL_CMAT_USE_ACCUMULATOR && params->wave_size == 32 ? 8 : 4);
+         base_row = nir_imul_imm(b, base_row, 4);
       }
    } else {
       base_row = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? nir_udiv_imm(b, local_idx, 16) : nir_imm_int(b, 0);
@@ -181,6 +183,23 @@
    return base_row;
 }
 
+static unsigned
+radv_get_row_iter(struct glsl_cmat_description desc, const lower_cmat_params *params, unsigned i)
+{
+   if (params->gfx_level >= GFX12) {
+      /* 8bit and ACC are indexed normally, 16bit A/B is weird. */
+      if (desc.use != GLSL_CMAT_USE_ACCUMULATOR && params->wave_size == 32 && radv_nir_cmat_bits(desc) >= 16)
+         return i + (i & 4);
+      else
+         return i;
+   } else {
+      if (desc.use != GLSL_CMAT_USE_ACCUMULATOR)
+         return i;
+      else
+         return i * params->wave_size / 16;
+   }
+}
+
 static nir_def *
 convert_base_type(nir_builder *b, nir_def *src, enum glsl_base_type src_type, enum glsl_base_type dst_type)
 {
@@ -193,6 +212,12 @@
    } else if (dst_type == GLSL_TYPE_BFLOAT16) {
       src = convert_base_type(b, src, src_type, GLSL_TYPE_FLOAT);
       return nir_f2bf(b, src);
+   } else if (src_type == GLSL_TYPE_FLOAT_E4M3FN) {
+      src = nir_e4m3fn2f(b, src);
+      return convert_base_type(b, src, GLSL_TYPE_FLOAT, dst_type);
+   } else if (dst_type == GLSL_TYPE_FLOAT_E4M3FN) {
+      src = convert_base_type(b, src, src_type, GLSL_TYPE_FLOAT);
+      return nir_f2e4m3fn(b, src);
    }
 
    nir_op op = nir_type_conversion_op(nir_get_nir_type_for_glsl_base_type(src_type),
@@ -201,6 +226,44 @@
    return nir_build_alu1(b, op, src);
 }
 
+static nir_def *
+radv_swizzle_gfx12_8bit_mat(nir_builder *b, nir_def *src, unsigned wave_size)
+{
+   assert(src->bit_size == 8);
+
+   src = nir_extract_bits(b, &src, 1, 0, src->num_components / 4, 32);
+
+   nir_def *res;
+
+   if (wave_size == 64) {
+      assert(src->num_components == 1);
+
+      nir_def *swapped = nir_rotate(b, src, nir_imm_int(b, 32), .cluster_size = 64);
+      swapped = nir_rotate(b, swapped, nir_imm_int(b, 16), .cluster_size = 32);
+
+      nir_def *cond = nir_inverse_ballot(b, 1, nir_imm_intN_t(b, 0xffffffff0000ull, 64));
+
+      res = nir_bcsel(b, cond, swapped, src);
+   } else {
+      assert(src->num_components == 2);
+
+      nir_def *src0 = nir_channel(b, src, 0);
+      nir_def *src1 = nir_channel(b, src, 1);
+
+      nir_def *swapped0 = nir_rotate(b, src0, nir_imm_int(b, 16), .cluster_size = 32);
+      nir_def *swapped1 = nir_rotate(b, src1, nir_imm_int(b, 16), .cluster_size = 32);
+
+      nir_def *cond = nir_inverse_ballot(b, 1, nir_imm_intN_t(b, 0xffff0000, 32));
+
+      nir_def *res0 = nir_bcsel(b, cond, swapped1, src0);
+      nir_def *res1 = nir_bcsel(b, cond, swapped0, src1);
+
+      res = nir_vec2(b, res0, res1);
+   }
+
+   return nir_extract_bits(b, &res, 1, 0, res->num_components * 4, 8);
+}
+
 bool
 radv_nir_lower_cooperative_matrix(nir_shader *shader, enum amd_gfx_level gfx_level, unsigned wave_size)
 {
@@ -311,7 +374,6 @@
 
                unsigned length = radv_nir_cmat_length(desc, &params);
                unsigned mul = radv_nir_cmat_length_mul(desc, &params);
-               unsigned lanes_per_iter = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? params.wave_size : 16;
                nir_def *vars[16];
                if (mul > 1) {
                   for (unsigned i = 0; i < length; ++i)
@@ -324,16 +386,10 @@
 
                for (unsigned i = 0; i < length / mul; ++i) {
                   nir_def *col_offset = inner_idx;
-                  nir_def *row_offset;
-                  uint32_t row_iter;
 
-                  if (gfx_level >= GFX12) {
-                     row_iter = desc.use != GLSL_CMAT_USE_ACCUMULATOR && wave_size == 32 ? i + (i & 4) : i;
-                  } else {
-                     row_iter = i * lanes_per_iter / 16;
-                  }
+                  uint32_t row_iter = radv_get_row_iter(desc, &params, i);
 
-                  row_offset = nir_iadd_imm(&b, base_row, row_iter);
+                  nir_def *row_offset = nir_iadd_imm(&b, base_row, row_iter);
 
                   if (layout == GLSL_MATRIX_LAYOUT_ROW_MAJOR) {
                      nir_def *tmp = col_offset;
@@ -385,7 +441,6 @@
 
                unsigned length = radv_nir_cmat_length(desc, &params);
                unsigned mul = radv_nir_cmat_length_mul(desc, &params);
-               unsigned lanes_per_iter = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? params.wave_size : 16;
                nir_def *vars[16];
                for (unsigned i = 0; i < length; ++i)
                   vars[i] = nir_channel(&b, src, i);
@@ -395,16 +450,10 @@
 
                for (unsigned i = 0; i < length / mul; ++i) {
                   nir_def *col_offset = inner_idx;
-                  nir_def *row_offset;
-                  uint32_t row_iter;
 
-                  if (gfx_level >= GFX12) {
-                     row_iter = desc.use != GLSL_CMAT_USE_ACCUMULATOR && wave_size == 32 ? i + (i & 4) : i;
-                  } else {
-                     row_iter = i * lanes_per_iter / 16;
-                  }
+                  uint32_t row_iter = radv_get_row_iter(desc, &params, i);
 
-                  row_offset = nir_iadd_imm(&b, base_row, row_iter);
+                  nir_def *row_offset = nir_iadd_imm(&b, base_row, row_iter);
 
                   if (layout == GLSL_MATRIX_LAYOUT_ROW_MAJOR) {
                      nir_def *tmp = col_offset;
@@ -437,8 +486,8 @@
                nir_def *B = radv_nir_load_cmat(&b, &params, intr->src[2].ssa);
                nir_def *C = radv_nir_load_cmat(&b, &params, intr->src[3].ssa);
 
-               nir_deref_instr *a_deref = nir_src_as_deref(intr->src[1]);
-               nir_deref_instr *b_deref = nir_src_as_deref(intr->src[2]);
+               nir_deref_instr *a_deref = nir_instr_as_deref(intr->src[1].ssa->parent_instr);
+               nir_deref_instr *b_deref = nir_instr_as_deref(intr->src[2].ssa->parent_instr);
                struct glsl_cmat_description a_desc = *glsl_get_cmat_description(a_deref->type);
                struct glsl_cmat_description b_desc = *glsl_get_cmat_description(b_deref->type);
 
@@ -452,7 +501,7 @@
                nir_def *ret = nir_cmat_muladd_amd(&b, A, B, C, .saturate = nir_intrinsic_saturate(intr),
                                                   .src_base_type = a_element_type, .src_base_type2 = b_element_type);
 
-               nir_deref_instr *dst_deref = nir_src_as_deref(intr->src[0]);
+               nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
                nir_store_deref(&b, dst_deref, ret, nir_component_mask(ret->num_components));
                nir_instr_remove(instr);
                progress = true;
@@ -484,8 +533,16 @@
                   src = nir_vec(&b, components, src->num_components / scale);
                }
 
+               if (dst_desc.use != GLSL_CMAT_USE_ACCUMULATOR && gfx_level >= GFX12 &&
+                   radv_nir_cmat_bits(src_desc) == 8 && radv_nir_cmat_bits(dst_desc) > 8)
+                  src = radv_swizzle_gfx12_8bit_mat(&b, src, wave_size);
+
                nir_def *ret = convert_base_type(&b, src, src_element_type, dst_element_type);
 
+               if (dst_desc.use != GLSL_CMAT_USE_ACCUMULATOR && gfx_level >= GFX12 &&
+                   radv_nir_cmat_bits(dst_desc) == 8 && radv_nir_cmat_bits(src_desc) > 8)
+                  ret = radv_swizzle_gfx12_8bit_mat(&b, ret, wave_size);
+
                if (dst_mul > src_mul) {
                   nir_def *components[NIR_MAX_VEC_COMPONENTS];
                   unsigned scale = dst_mul / src_mul;
--- mesa-chaotic/src/amd/vulkan/radv_physical_device.c	2025-05-11 17:14:25.536063876 +0200
+++ mesa-dadschoorse/src/amd/vulkan/radv_physical_device.c	2025-05-11 17:09:56.120770164 +0200
@@ -574,12 +574,11 @@
       .KHR_ray_tracing_pipeline = radv_enable_rt(pdev),
       .KHR_ray_tracing_position_fetch = radv_enable_rt(pdev),
       .KHR_relaxed_block_layout = true,
-      .KHR_robustness2 = true,
       .KHR_sampler_mirror_clamp_to_edge = true,
       .KHR_sampler_ycbcr_conversion = true,
       .KHR_separate_depth_stencil_layouts = true,
       .KHR_shader_atomic_int64 = true,
-      .KHR_shader_bfloat16 = pdev->info.gfx_level >= GFX12, /* GFX11 has precision issues. */
+      .KHR_shader_bfloat16 = pdev->info.gfx_level >= GFX11,
       .KHR_shader_clock = true,
       .KHR_shader_draw_parameters = true,
       .KHR_shader_expect_assume = true,
@@ -971,7 +970,7 @@
       .stippledBresenhamLines = true,
       .stippledSmoothLines = false,
 
-      /* VK_KHR_robustness2 */
+      /* VK_EXT_robustness2 */
       .robustBufferAccess2 = true,
       .robustImageAccess2 = true,
       .nullDescriptor = true,
@@ -1309,7 +1308,7 @@
 
       /* VK_KHR_shader_bfloat16 */
       .shaderBFloat16Type = true,
-      .shaderBFloat16DotProduct = true,
+      .shaderBFloat16DotProduct = pdev->info.gfx_level >= GFX12, /* v_dot2_bf16_bf16 isn't working on GFX11. */
       .shaderBFloat16CooperativeMatrix = radv_cooperative_matrix_enabled(pdev),
    };
 }
@@ -1749,7 +1748,7 @@
       .sampleLocationSubPixelBits = 4,
       .variableSampleLocations = true,
 
-      /* VK_KHR_robustness2 */
+      /* VK_EXT_robustness2 */
       .robustStorageBufferAccessSizeAlignment = 4,
       .robustUniformBufferAccessSizeAlignment = 4,
 
@@ -2077,23 +2076,19 @@
 
 #ifdef _WIN32
    pdev->ws = radv_null_winsys_create();
-   if (!pdev->ws)
-      result = VK_ERROR_OUT_OF_HOST_MEMORY;
 #else
    if (drm_device) {
       bool reserve_vmid = instance->vk.trace_mode & RADV_TRACE_MODE_RGP;
 
-      result = radv_amdgpu_winsys_create(fd, instance->debug_flags, instance->perftest_flags, reserve_vmid, is_virtio,
-                                         &pdev->ws);
+      pdev->ws =
+         radv_amdgpu_winsys_create(fd, instance->debug_flags, instance->perftest_flags, reserve_vmid, is_virtio);
    } else {
       pdev->ws = radv_null_winsys_create();
-      if (!pdev->ws)
-         result = VK_ERROR_OUT_OF_HOST_MEMORY;
    }
 #endif
 
-   if (result != VK_SUCCESS) {
-      result = vk_errorf(instance, result, "failed to initialize winsys");
+   if (!pdev->ws) {
+      result = vk_errorf(instance, VK_ERROR_INITIALIZATION_FAILED, "failed to initialize winsys");
       goto fail_base;
    }
 
@@ -2859,8 +2854,8 @@
          VkComponentTypeKHR ab_type = bfloat ? VK_COMPONENT_TYPE_BFLOAT16_KHR : VK_COMPONENT_TYPE_FLOAT16_KHR;
          VkComponentTypeKHR cd_type = fp32 ? VK_COMPONENT_TYPE_FLOAT32_KHR : ab_type;
 
-         if (pdev->info.gfx_level < GFX12 && bfloat)
-            continue; /* BF16 isn't working precisely on GFX11. */
+         if (pdev->info.gfx_level < GFX12 && bfloat && !fp32)
+            continue; /* BF16 accumulator isn't working correctly on GFX11. */
 
          vk_outarray_append_typed(VkCooperativeMatrixPropertiesKHR, &out, p)
          {
--- mesa-chaotic/src/amd/vulkan/radv_pipeline.c	2025-05-11 17:14:25.536063876 +0200
+++ mesa-dadschoorse/src/amd/vulkan/radv_pipeline.c	2025-05-11 17:09:56.120770164 +0200
@@ -265,6 +265,10 @@
       return 1;
 
    const nir_alu_instr *alu = nir_instr_as_alu(instr);
+
+   if (alu->op == nir_op_f2e4m3fn || alu->op == nir_op_e4m3fn2f)
+      return 2;
+
    const unsigned bit_size = alu->def.bit_size;
    if (bit_size != 16)
       return 1;
--- mesa-chaotic/src/amd/vulkan/radv_shader.c	2025-05-11 17:14:25.538063846 +0200
+++ mesa-dadschoorse/src/amd/vulkan/radv_shader.c	2025-05-11 17:09:56.122770122 +0200
@@ -82,6 +82,10 @@
       return 0;
 
    const nir_alu_instr *alu = nir_instr_as_alu(instr);
+
+   if (alu->op == nir_op_f2e4m3fn || alu->op == nir_op_e4m3fn2f)
+      return 2;
+
    const unsigned bit_size = alu->def.bit_size;
    if (bit_size == 16)
       return 2;
@@ -586,13 +590,6 @@
 
    NIR_PASS(_, nir, nir_lower_load_const_to_scalar);
    NIR_PASS(_, nir, nir_opt_shrink_stores, !instance->drirc.disable_shrink_image_store);
-   if (nir->info.stage == MESA_SHADER_FRAGMENT && nir->info.fs.uses_discard)
-      NIR_PASS(_, nir, nir_lower_discard_if, nir_move_terminate_out_of_loops);
-
-   const nir_opt_access_options opt_access_options = {
-      .is_vulkan = true,
-   };
-   NIR_PASS(_, nir, nir_opt_access, &opt_access_options);
 
    if (!stage->key.optimisations_disabled)
       radv_optimize_nir(nir, false);
@@ -611,6 +608,11 @@
          NIR_PASS(_, nir, nir_opt_constant_folding);
    }
 
+   const nir_opt_access_options opt_access_options = {
+      .is_vulkan = true,
+   };
+   NIR_PASS(_, nir, nir_opt_access, &opt_access_options);
+
    NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_push_const, nir_address_format_32bit_offset);
 
    NIR_PASS(_, nir, nir_lower_explicit_io, nir_var_mem_ubo | nir_var_mem_ssbo,
@@ -1955,10 +1957,7 @@
    const struct radv_shader_info *info = &binary->info;
    gl_shader_stage stage = binary->info.stage;
    bool scratch_enabled = config->scratch_bytes_per_wave > 0;
-   const bool trap_enabled = !!device->trap_handler_shader;
-   /* On GFX12, TRAP_PRESENT doesn't exist for compute shaders and it's enabled by default. */
-   const enum ac_hw_stage hw_stage = radv_select_hw_stage(info, pdev->info.gfx_level);
-   const bool trap_present = trap_enabled && (pdev->info.gfx_level < GFX12 || hw_stage != AC_HW_COMPUTE_SHADER);
+   bool trap_enabled = !!device->trap_handler_shader;
    unsigned vgpr_comp_cnt = 0;
    unsigned num_input_vgprs = args->ac.num_vgprs_used;
 
@@ -1982,7 +1981,7 @@
    config->num_shared_vgprs = num_shared_vgprs;
 
    config->rsrc2 = S_00B12C_USER_SGPR(args->num_user_sgprs) | S_00B12C_SCRATCH_EN(scratch_enabled) |
-                   S_00B12C_TRAP_PRESENT(trap_present);
+                   S_00B12C_TRAP_PRESENT(trap_enabled);
 
    if (trap_enabled) {
       /* Configure the shader exceptions like memory violation, etc. */
@@ -2140,7 +2139,8 @@
    case MESA_SHADER_ANY_HIT:
    case MESA_SHADER_COMPUTE:
    case MESA_SHADER_TASK:
-      config->rsrc1 |= S_00B848_MEM_ORDERED(radv_mem_ordered(pdev)) | S_00B848_WGP_MODE(wgp_mode);
+      config->rsrc1 |= S_00B848_MEM_ORDERED(radv_mem_ordered(pdev)) | S_00B848_WGP_MODE(wgp_mode) |
+                       S_00B848_FP16_OVFL(info->uses_f2e4m3fn);
       config->rsrc2 |= S_00B84C_TGID_X_EN(info->cs.uses_block_id[0]) | S_00B84C_TGID_Y_EN(info->cs.uses_block_id[1]) |
                        S_00B84C_TGID_Z_EN(info->cs.uses_block_id[2]) |
                        S_00B84C_TIDIG_COMP_CNT(info->cs.uses_thread_id[2]   ? 2
--- mesa-chaotic/src/amd/vulkan/radv_shader_info.c	2025-05-11 17:14:25.540063816 +0200
+++ mesa-dadschoorse/src/amd/vulkan/radv_shader_info.c	2025-05-11 17:09:56.122770122 +0200
@@ -346,6 +346,18 @@
 }
 
 static void
+gather_alu_info(const nir_shader *nir, const nir_alu_instr *instr, struct radv_shader_info *info)
+{
+   switch (instr->op) {
+   case nir_op_f2e4m3fn:
+      info->uses_f2e4m3fn = true;
+      break;
+   default:
+      break;
+   }
+}
+
+static void
 gather_info_block(const nir_shader *nir, const nir_block *block, struct radv_shader_info *info,
                   const struct radv_graphics_state_key *gfx_state, const struct radv_shader_stage_key *stage_key,
                   bool consider_force_vrs)
@@ -358,6 +370,8 @@
       case nir_instr_type_tex:
          gather_tex_info(nir, nir_instr_as_tex(instr), info);
          break;
+      case nir_instr_type_alu:
+         gather_alu_info(nir, nir_instr_as_alu(instr), info);
       default:
          break;
       }
@@ -998,8 +1012,7 @@
       info->ps.spi_shader_col_format = gfx_state->ps.epilog.spi_shader_col_format;
 
       /* Clear color attachments that aren't exported by the FS to match IO shader arguments. */
-      if (!info->ps.mrt0_is_dual_src)
-         info->ps.spi_shader_col_format &= info->ps.colors_written;
+      info->ps.spi_shader_col_format &= info->ps.colors_written;
 
       info->ps.cb_shader_mask = ac_get_cb_shader_mask(info->ps.spi_shader_col_format);
    }
@@ -1800,16 +1813,14 @@
                vs_stage->nir->info.float_controls_execution_mode == tcs_stage->nir->info.float_controls_execution_mode;
 
             if (vs_stage->info.vs.tcs_in_out_eq) {
-               vs_stage->info.vs.tcs_inputs_via_temp =
-                  vs_stage->nir->info.outputs_written &
-                  ~(vs_stage->nir->info.outputs_read_indirectly | vs_stage->nir->info.outputs_written_indirectly) &
-                  tcs_stage->nir->info.tess.tcs_same_invocation_inputs_read;
-               vs_stage->info.vs.tcs_inputs_via_lds =
-                  tcs_stage->nir->info.tess.tcs_cross_invocation_inputs_read |
-                  (tcs_stage->nir->info.tess.tcs_same_invocation_inputs_read &
-                   tcs_stage->nir->info.inputs_read_indirectly) |
-                  (tcs_stage->nir->info.tess.tcs_same_invocation_inputs_read &
-                   (vs_stage->nir->info.outputs_read_indirectly | vs_stage->nir->info.outputs_written_indirectly));
+               vs_stage->info.vs.tcs_inputs_via_temp = vs_stage->nir->info.outputs_written &
+                                                       ~vs_stage->nir->info.outputs_accessed_indirectly &
+                                                       tcs_stage->nir->info.tess.tcs_same_invocation_inputs_read;
+               vs_stage->info.vs.tcs_inputs_via_lds = tcs_stage->nir->info.tess.tcs_cross_invocation_inputs_read |
+                                                      (tcs_stage->nir->info.tess.tcs_same_invocation_inputs_read &
+                                                       tcs_stage->nir->info.inputs_read_indirectly) |
+                                                      (tcs_stage->nir->info.tess.tcs_same_invocation_inputs_read &
+                                                       vs_stage->nir->info.outputs_accessed_indirectly);
             }
          }
       }
@@ -1845,6 +1856,7 @@
    dst_info->desc_set_used_mask |= src_info->desc_set_used_mask;
    dst_info->uses_view_index |= src_info->uses_view_index;
    dst_info->uses_prim_id |= src_info->uses_prim_id;
+   dst_info->uses_f2e4m3fn |= src_info->uses_f2e4m3fn;
    dst_info->inline_push_constant_mask |= src_info->inline_push_constant_mask;
 
    /* Only inline all push constants if both allows it. */
--- mesa-chaotic/src/amd/vulkan/radv_shader_info.h	2025-05-11 17:14:25.540063816 +0200
+++ mesa-dadschoorse/src/amd/vulkan/radv_shader_info.h	2025-05-11 17:09:56.122770122 +0200
@@ -89,6 +89,7 @@
    bool uses_view_index;
    bool uses_invocation_id;
    bool uses_prim_id;
+   bool uses_f2e4m3fn;
    uint8_t wave_size;
    uint8_t ballot_bit_size;
    struct radv_userdata_locations user_sgprs_locs;
